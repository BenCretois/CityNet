{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import csv\n",
    "import collections\n",
    "\n",
    "import scipy.io.wavfile\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "import sys, os\n",
    "sys.path.append('/Users/Michael/projects/engage/engaged_hackathon/')\n",
    "from engaged.features import features as engaged_features\n",
    "from engaged.features import frequency\n",
    "\n",
    "from features import mfcc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "base_path = '/Users/Michael/projects/engage/engaged_hackathon_data/cropped_audio/'\n",
    "\n",
    "spectrogram_parameters = {\n",
    "    'nfft': 1024,\n",
    "    'window_width': 0.03,\n",
    "    'overlap': 0.01\n",
    "    }\n",
    "\n",
    "# we won't bother training OR testing on classes with fewer data than this number:\n",
    "minimum_class_count = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Counting the number of each class in the dataset\n",
    "Here we loop through the dataset once to get the count of each class. See also the `dataset_overview` notebook for a proper analysis of class frequency in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "filelist = csv.reader(open(base_path + 'below12kHz_cropped_label.txt'))\n",
    "\n",
    "# loop over each wav file\n",
    "Y = []\n",
    "for f in filelist:\n",
    "    wavpath, start_time, wav_class, _ = f\n",
    "    Y.append(wav_class.lower().strip())\n",
    "\n",
    "# Make use of the collections counter class\n",
    "class_counts = collections.Counter(Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading data and computing different types of features\n",
    "For each wav file in the dataset, compute a few different types of features. Instead of loading all the data into memory first, each wav file is loaded one at a time. This reduces the memory load at the expense of repeating I/O operations if the notebook is run multiple times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 100 200 300 400 500 600 700 800 900 1000 1100 1200 1300 1400 1500 1600 1700 1800 Skipping  music\n",
      "1900 2000 2100 2200 Skipping  horn\n",
      "2300 Skipping  invertebrate\n",
      "Skipping  invertebrate\n",
      "Skipping  invertebrate\n",
      "Skipping  invertebrate\n",
      "Skipping  music\n",
      "2400 2500 2600 Skipping  applause\n",
      "2700 2800 Skipping  invertebrate\n",
      "2900 3000 3100 3200 3300 3400 3500 3600 3700 3800 3900 4000 4100 4200 4300 4400 4500 Skipping  invertebrate\n",
      "Skipping  invertebrate\n",
      "4600 4700 4800 4900 5000 5100 5200 5300 5400 Skipping  siren\n",
      "Skipping  siren\n",
      "Skipping  siren\n",
      "5500 Skipping  siren\n",
      "Skipping  siren\n",
      "Skipping  siren\n",
      "5600 5700 5800 5900 6000 6100 6200 6300 6400 6500 6600 6700 6800 6900 7000 7100 7200 Skipping  startthecar\n",
      "7300 7400 7500 7600 7700 7800 7900 8000 8100 8200 8300 Skipping  music\n",
      "8400 8500 Skipping  coughing\n",
      "Skipping  coughing\n",
      "Skipping  coughing\n",
      "Skipping  coughing\n",
      "Skipping  coughing\n",
      "8600 Skipping  bells\n",
      "Skipping  coughing\n",
      "Skipping  coughing\n",
      "8700 Skipping  siren\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Michael/anaconda/lib/python2.7/site-packages/scipy/ndimage/interpolation.py:549: UserWarning: From scipy 0.13.0, the output shape of zoom() is calculated with round() instead of int() - for these inputs the size of the returned array has changed.\n",
      "  \"the returned array has changed.\", UserWarning)\n"
     ]
    }
   ],
   "source": [
    "# list of class labels\n",
    "Y = []  \n",
    "\n",
    "# Features. We maintain a separate list for each feature\n",
    "X_mfcc = []\n",
    "X_resized_spectrogram = []\n",
    "X_max_bins = []\n",
    "\n",
    "filelist = csv.reader(open(base_path + 'below12kHz_cropped_label.txt'))\n",
    "\n",
    "# loop over each wav file\n",
    "for count, f in enumerate(filelist):\n",
    "    \n",
    "    wavpath, start_time, wav_class, _ = f\n",
    "    \n",
    "    # don't bother with this example if there aren't enough of them in the dataset\n",
    "    if class_counts[wav_class.lower().strip()] < minimum_class_count:\n",
    "        print \"Skipping \", wav_class.lower().strip()\n",
    "        continue\n",
    "    \n",
    "    # add to the list of classes\n",
    "    Y.append(wav_class.lower().strip())\n",
    "    \n",
    "    # load in sound file and convert to spectogram  \n",
    "    sr, wav = scipy.io.wavfile.read(base_path + wavpath)  \n",
    "    spec = frequency.spectrogram(wav, sr, **spectrogram_parameters)\n",
    "    \n",
    "    # ---------------------------------------------\n",
    "    # FEATURE COMPUTATION\n",
    "    \n",
    "    # Small spectrogram: Simply make the spectrogram small, each pixel is a feature bin\n",
    "    X_resized_spectrogram.append(engaged_features.small_spectrogram(spec))\n",
    "    \n",
    "    # Max bins: Find the maximum frequency at each time step\n",
    "    X_max_bins.append(engaged_features.max_bins(spec))\n",
    "    \n",
    "    # MFCC features\n",
    "    mfcc_feats = mfcc(wav, sr, ceplifter=0, preemph=0, numcep=13)\n",
    "    accumulated_features = np.hstack([mfcc_feats.mean(0), mfcc_feats.var(0), mfcc_feats.max(0)])\n",
    "    X_mfcc.append(accumulated_features)\n",
    "    \n",
    "    # ---------------------------------------------\n",
    "   \n",
    "    # Keep us updated on progress\n",
    "    if count % 100 == 0:\n",
    "        print count, \n",
    "        \n",
    "    # Uncomment this to do a 'small' run with only a few examples\n",
    "#     if count >= 500:\n",
    "#         break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combining features into Numpy arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature matrix \"resized_spectrogram\" has shape (8737, 100)\n",
      "Feature matrix \"max_bins\" has shape (8737, 20)\n",
      "Feature matrix \"mfcc\" has shape (8737, 39)\n"
     ]
    }
   ],
   "source": [
    "# combine the lists of features into numpy arrays of features\n",
    "X = {}\n",
    "X['mfcc'] = np.vstack(X_mfcc)\n",
    "X['resized_spectrogram'] = np.vstack(X_resized_spectrogram)\n",
    "X['max_bins'] = np.vstack(X_max_bins)\n",
    "\n",
    "for name, x in X.iteritems():\n",
    "    print \"Feature matrix \\\"%s\\\" has shape (%d, %d)\" % (name, x.shape[0], x.shape[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training and testing the models for each feature type\n",
    "The aim is to see how good each set of features is for classification.\n",
    "We run a different experiment for each set of features. We also run an experiment which combines all the features together.\n",
    "To remove dependancy on the train-test split used, we repeat this for 50 different train test splits.\n",
    "\n",
    "To account for imbalance in the training set, we use the `class_weight` functionality of `sklearn.ensemble.RandomForestClassifier`. This was introduced in sklearn 0.16.\n",
    "\n",
    "\n",
    "**TODO - we should really split by file not just by instance**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seed 0 Seed 1 Seed 2 Seed 3 Seed 4 Seed 5 Seed 6 Seed 7 Seed 8 Seed 9 Seed 10 Seed 11 Seed 12 Seed 13 Seed 14 Seed 15 Seed 16 Seed 17 Seed 18 Seed 19 Seed 20 Seed 21 Seed 22 Seed 23 Seed 24 Seed 25 Seed 26 Seed 27 Seed 28 Seed 29 Seed 30 Seed 31 Seed 32 Seed 33 Seed 34 Seed 35 Seed 36 Seed 37 Seed 38 Seed 39 Seed 40 Seed 41 Seed 42 Seed 43 Seed 44 Seed 45 Seed 46 Seed 47 Seed 48 Seed 49\n"
     ]
    }
   ],
   "source": [
    "# setting up a dictionary of experiments to run\n",
    "# keys are names of this experiment\n",
    "# values are a list of all the features to test on for this experiment\n",
    "experiments = {\n",
    "    'mfcc': ['mfcc'],\n",
    "    'resized_spectrogram': ['resized_spectrogram'],\n",
    "    'max_bins': ['max_bins'],\n",
    "    'all': ['mfcc', 'resized_spectrogram', 'max_bins']\n",
    "    }\n",
    "\n",
    "# set up a dictionary to store the results.\n",
    "# Using DefaultDict: https://docs.python.org/2/library/collections.html#defaultdict-examples\n",
    "results = collections.defaultdict(list)\n",
    "\n",
    "# we will repeat the experiment for multiple train/test splits, and ultimately average over them\n",
    "for random_seed in range(50):\n",
    "    \n",
    "    print \"Seed\", random_seed,\n",
    "    \n",
    "    # TODO - we should really split by file not just by instance\n",
    "    train_idxs, test_idxs = train_test_split(range(len(Y)))\n",
    "    \n",
    "    Y_train = [Y[idx] for idx in train_idxs]\n",
    "    Y_test = [Y[idx] for idx in test_idxs]\n",
    "    \n",
    "    # Set a baseline where all the test set is classified as bird\n",
    "    all_bird = {'Y_pred': ['bird'] * len(Y_test), 'Y_test': Y_test}\n",
    "    results['classify_all_as_bird'].append(all_bird)\n",
    "            \n",
    "    # loop over each experiment\n",
    "    for experiment_name, feature_list in experiments.iteritems():\n",
    "\n",
    "        # concatenating all the feature vectors for this experiment together\n",
    "        X_np = np.hstack([X[feat_name] for feat_name in feature_list])\n",
    "        \n",
    "        X_train = X_np[train_idxs]\n",
    "        X_test = X_np[test_idxs]\n",
    "\n",
    "        # training a classifier on these features and then testing on test set\n",
    "        rf = RandomForestClassifier(n_estimators=100, max_depth=20, class_weight='auto')\n",
    "        rf.fit(X_train, Y_train)\n",
    "        Y_pred = rf.predict(X_test)\n",
    "                \n",
    "        # adding this result to the results dictionary\n",
    "        this_result = {'Y_test': Y_test, 'Y_pred': list(Y_pred)}\n",
    "        results[experiment_name].append(this_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating accuracy of approaches\n",
    "There are "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mfcc                     0.736\t 0.414\n",
      "all                      0.668\t 0.206\n",
      "max_bins                 0.582\t 0.077\n",
      "resized_spectrogram      0.556\t 0.070\n",
      "classify_all_as_bird     0.549\t 0.051\n"
     ]
    },
    {
     "data": {
      "image/png": [
       "iVBORw0KGgoAAAANSUhEUgAAAccAAAEKCAYAAABnip7YAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\n",
       "AAALEgAACxIB0t1+/AAAGAxJREFUeJzt3XmUbWV95vHvI1cFmRGXraKAaNQYpIFgnIhltJ2iOCCw\n",
       "NBKnEGI6iUlMNEubpoxjop12DMsQFeOUNoqKiigYygGRSbhcIGBoIcGoaeOFiEQBub/+47wlJy9V\n",
       "p07dW1Wnqu73s1at2nufPfzec07tp9599tk7VYUkSbrdnSZdgCRJq43hKElSx3CUJKljOEqS1DEc\n",
       "JUnqGI6SJHU2TLoAbbskfh9HkrZCVWWu6YbjOjHfC7xeJZmuqulJ17GSbPP2wTav6Hbn7Vh4WFWS\n",
       "pI7hKElSx3DUWjUz6QImYGbSBUzAzKQLmICZSRcwATOTLqAXr6269iWp7e0zR0naVqP2nfYcJUnq\n",
       "GI6SJHUMR0mSOoajJEkdw1GSpI7hKElbKWFzQpFUwuZJ16Ol41c51gG/yiFNRkJVEZIKRRX+Ha4h\n",
       "fpVDkqRFMBwlSeoYjpIkdQxHSVqEce6f6j1W1z7DUZKkjuE4YUnukuSsJBcnOWrS9UiSYMOkCxCH\n",
       "AFVVB0+6EEnSgD3HZZRkvyRXJnlfkquSfCjJE5N8Lck3kxwGfBA4rPUc75/ksCTnJLkkyXlJdk6y\n",
       "Q5K3JNmUZGOS35l02yRpPbPnuPwOAI4ErgAuAI6pqkclOQJ4FfAS4I+q6ulJ7gKcCRxdVRcl2QX4\n",
       "CfCbwP2Ag6pqS5I9J9ISSdpOGI7L75qquhwgyeXAWW36ZcB+3bwPAr5bVRcBVNWP2nKPB06qqi1t\n",
       "+vX9RpJMD43OVNXM0jVB0rCEOc9GnW+6VockU8DUOPMajsvv5qHhLcAtQ8OLef5HXpaqqqYXV5ak\n",
       "rTV7mbg+DOebrtWhdRpmZseTnDjfvH7muLpcBdwryS8CJNk1yQ4MDrUe34bxsKokLS/Dcfn1/0HW\n",
       "HMMFUFW3AMcA70hyCfB54K7AXwP/DFzapj93WSuWpO2cd+VYB7wrh7Ryhv/e5rsrh3+Ta4N35ZAk\n",
       "aREMR0mSOoajJC3COIdLPaS69hmOkiR1DEdJkjqGoyRJHcNRkrbB0NVw7nBZR61dXj5OkrbS7Pca\n",
       "obxe3Dpjz1GSpI7hKElSx3CUJKljOEqS1DEcJUnqGI6SJHUMR0mSOoajJEkdw1GSpI7hKElSx3CU\n",
       "JKljOEqS1DEcJUnqGI6SJHUMR0mSOoajJEkdw1GSpI7hKElSx3CUJKljOEqS1DEcJW0XEjYnFEkl\n",
       "VMLmSdek1WvDpAuQpBWyZxUhVBVJqEkXpNXLnqMkSR3DUZKkjuEoSVLHcJQkqWM4Slq3kox90s1i\n",
       "5tX6ZziuAUmuTbJXG/7RpOuRpPXOcFwbap5hSdIyMBxXmSSfSHJhksuSHDfpeqS1JmFq0jVo7fMi\n",
       "AKvPi6vq+iQ7Aecn+fikC5LWmClgZsI1aI0zHFeflyV5ZhveB3jgJIuRpO2R4biKJJkCHg88oqp+\n",
       "kuRsYMcxl50eGp2pqpklL1Bapdqh1Kk2emIy+8hjGXWZOC8ht31p+9ipseat8r2xWiQ5AviNqjoi\n",
       "yUOAbwBPBk4BDq2qzUlurKpdu+WqqnLHNUrbn4TpKqYHw7f/baRdU5WkqMrPxn+2nH9H25tRr7kn\n",
       "5KwuZwAbklwBvAE4t033bFVJWkH2HNcB/+OVbpcwVTU4Iceeo0YZ9ZobjuuAf9TS3AxHjeJhVUmS\n",
       "FsFwlLRuLaYnaK9RwwxHSZI6hqMkSR3DUZKkjuEoSVLHcJS03Zi9XFz7ff2Ey9Eq5rVVJW0Xbv9O\n",
       "Y3mZKS3InqMkSR3DUZKkjuEoSVLHcJQkqWM4SpLUMRwlSeoYjpIkdQxHSZI6hqMkSR3DUZKkjuEo\n",
       "SVLHcJQkqWM4SpLUMRwlSeoYjpIkdQxHSZI6hqMkSR3DUZKkjuEoSVLHcJQkqWM4SpqohM0JRVIJ\n",
       "myddjwSQqpp0DdpGSaqqMuk6pK2RUFWEpEJRhe9lrYhR+057jpIkdQxHSZI6hqMkSR3DUZKkjuEo\n",
       "acUlGXkm4EKPS8vNcFwBSaaTvHyO6fdO8neTqEmSND/DcWXM+V9wVX2nqo5a6WKklZYwNdfwOMsl\n",
       "/P5y1CSNsl2HY5L9klyZ5H1JrkryoSRPTHJOkm8mOaz9fC3JN9r0n2vL/kGS97ThA5NsSrLjiM0d\n",
       "1NbzzSS/MbT9TW34hUlOTfK5Ns+ftek7JDmlrf/SJO4otBZNzTM8znLPXMpCpHFsmHQBq8ABwJHA\n",
       "FcAFwDFV9egkRwCvAo4FDq+q25I8AXgD8BzgrcBMkme1+X6zqn4yzzYCPAz4JWAX4OIkn5ljvoOA\n",
       "/wrcAlyV5B3APYF7V9WBAEl2X4pGS5LmZzjCNVV1OUCSy4Gz2vTLgP2APYAPJHkAg8Ojdwaoqkry\n",
       "QmATcFJVnTtiGwV8sqpuBm5OcjaDoNzYzffFqrqx1XIFcD8GoX3/JG8HPgt8Ya4NJJkeGp2pqpkF\n",
       "Wy4to3b4dKqNnjh0OPWxcCLJ3B83DKZ/joRrgX3btGuBG4BTqnjrMpatdSzJFGMeuTAc4eah4S0M\n",
       "em2zwxuA1zIIrWcl2ReYGZr/54AbgftsxXa3LFDLbcCGqrohyUHAk4DfAo4GXtIvWFXTW1GDtGyq\n",
       "mKH9vSRQxXQbnobXnFg1nTZe3XJJnlJVtd9gXqaqFnUoVppT6zTMzI4nOXG+ebfrzxzHEGA34Dtt\n",
       "/EU/e2BwePNtwOHA3ZMcucB6npHkrknuzuA/lwvG2X6bf4eqOhU4AThk0a2QJC2K4XjHM0mHx7cA\n",
       "bwbemOQbwA5Dj/8F8M6quppBT+5NSfYesY1LgbOBc4E/rarvddureWq5D3B2kouBDwB/soi2SavF\n",
       "zDzD4yz3yaUsRBqHd+VYB7wrh9aa4ffsXHfl8D2tleBdOSRJWgRPyFlC7ezVl3WTv1pVvzuBciRJ\n",
       "W8nDquuAh6C0lnmzY02Kh1UlSVoEw1GSpI7hKElSx3CUNHFDV8m5fqKFSI1nq0qaqNtPwKm5L7Yq\n",
       "TYA9R0mSOoajJEkdw1GSpI7hKElSx3CUJKljOEqS1DEcJUnqGI6SJHUMR0mSOoajJEkdw1GSpI7h\n",
       "KElSx3CUJKljOEqS1DEcJUnqGI6SJHUMR0mSOoajJEkdw1GSpI7hKElSx3CUJKljOEpaNgmbE4qk\n",
       "EmZ/Nk+6LmkhGyZdgKR1bc8qQqgqApBQky5KWog9R0mSOoajJEkdw1GSpI7hKGlJJRn7M8XFzCut\n",
       "JMNRkqTOsoVjkpOTPGQb17Ffkk1LVdMit71vkudOYtuSpMkaOxzTjDt/VR1XVf+wdWWtCvsDz5vr\n",
       "gSTb9BWYxT6XkqSVNTIcW8/tqiTvBzYBJyQ5P8nGJNNtnp2TfDbJJUk2JTmqTZ9JcmiSpye5uP1c\n",
       "leRb7fFD2zwXJjkjyX8Zmr4xySXAby9Q30OTnNfWvTHJAa3mK5N8MMkVSf4uyU4LbPMBSc5qbbgw\n",
       "yf2BNwGHt3X/fpIXJDktyReBM5PsmeSTbbvnJjmwreseSc5MclnrPV+bZK85nsv7JvnLJBe0eaeH\n",
       "2nVtkje0bV+Y5JAkX0hydZLjF/siS5IWqarm/QH2A24DHg78N+DdbfqdgE8DhwPPBv5qaJnd2u+z\n",
       "gUO69f0f4KUMLj7wNeDubfoxwHva8KXAY9rwnwObRtT3duB5bXgDsGOreQvwyDb9PcDLF9jmecAz\n",
       "2vBdgJ2AxwKfHtrWC4HrgD3a+DuAE9rw44CL2/A7gVe24Se1WvYafi6H1rln+71De75+oY1fAxzf\n",
       "hv+iPSc7A3sD35vjeahRr6M//qzkz/D7cTBY1FzTyveuP5P9GfX+G+fw4D9V1flJ3gI8McnFbfrO\n",
       "wAOArwL/K8mbgM9U1VfnWkmSVwD/UVUnJfkF4KHAWe3o4g7Ad5LsDuw+tI4PAE8ZUdu5wKuT7AOc\n",
       "WlVXt/VdV1Xntnk+CPwecMY829wFuHdVfao9U7e0evvDngWcWVU3tPFHM/jHgKo6O8ndk+zapj+z\n",
       "Tf98kuv753Jo/JgkxzEI7nsBPw9c1h47rf3eBOxcVTcBNyW5OcluVfXD4eKGe57ATFXNjHjepGW1\n",
       "0FVwvEqOJiHJFDA1zrzjhONNQ8NvrKq/mmODBwO/CrwuyRer6rXd408AjgR+eXYScHlVPaqbb49+\n",
       "1aMKq6qPJPk68DTg9HbI8Rr4T394aePzbXPXUdvo3NSNz1fffNN/tnyS/Rn0aH+xqv49yfsY9Hxn\n",
       "3dx+bwFuGZq+hTlet6qanr9saWXVApeKW+hxaTm0TsPM7HiSE+ebdzFnq34eeHGSndtK79M+X7sX\n",
       "8JOq+hDwFuDg4YWS7Au8Czi6qmZ3+FcB90jyiDbPnZP8fOuV3ZDk0W2+XxtVUJL9q+qaqnoH8Cng\n",
       "wPbQ/WbXzeCkmq+M2OaNwLeTPKNNv2v7jPKHwHBw9oH3ldn62n8j32/rOgc4uk1/IrDnPOXvxiAs\n",
       "f5jknszfQ/bEHUlaYeP0HGcPzJ6ZwVczzm1HHG8EjmVwaPXNSbYAtwK/NbRsgBcw+Mztk225f6mq\n",
       "pyV5DvD2dih1A/C/gSuAFwHvzeDLwV+Akf9ZHp3k2Lbd7wKvB/ZgEIT/Pcl7gcuBk6rq1hHbPBZ4\n",
       "d5I/bet6DoPP+W5rJwadAlzf1TLd6tzIIORe0Ka/BvhIq+tc4HvtudptePmq2tgOUV/J4LPMOQ9H\n",
       "t2WqG5ckLaO0DyXXjST7MTiR5sAFZl2u7d8FuK2qbkvySOBdVXXIMm+zqsoeplaF4fdjZu/GkRT9\n",
       "NHzvarJGvf/W6y2rJpn49wM+muRODD4rPG6CtUiStsKa6DkmeRKD7x0O+1ZVHTmJelYb//vWarVQ\n",
       "z1GapFH7zjURjhrNcNRqZThqNRu17/TC45IkdQxHSZI6hqMkSZ31eraqpFUioYr/dDWc60fNL60G\n",
       "hqOkZXP7iTfl1Su0pnhYVZKkjuEoSVLHcJQkqWM4SpLUMRwlSeoYjpIkdQxHSZI6hqMkSR3DUZKk\n",
       "juEoSVLHcJQkqWM4SpLUMRwlSeoYjpIkdQxHSZI6hqMkSR3DUZKkjuEoSVLHcJQkqWM4SpLUMRwl\n",
       "LYuEzQlFUgmbJ12PtBipqknXoG2UpKoqk65DGpZQVYSkQlGF71GtKqP2nfYcJUnqGI6SJHUMR0mS\n",
       "OoajJEkdw1HSNksy9pl9i5lXmhTDUZKkzrKEY5LpJC9fwvWdMzT85iSXJfmzJVr3KUmObMMzSQ5d\n",
       "ivUutK1u+qFJ3jbmOq5NstfSVydJmrVhmda7pIdNqurRQ6PHAXvW0n1Bs7i93uHh5TDnuqvqIuCi\n",
       "fnqSDVX103HWIUlaOkvSc0zy60k2Jrkkyd90jx2X5Pz22MeS7NSmH5VkU5v+pTbtoUnOS3JxW98B\n",
       "bfqP2u/TgF2AbyR5RZKLhrbzwOHxOWo8odWxKcm7t7Kdf5nkgtZznR6a/qYkl7ea37zAap7Q1nFV\n",
       "kl9ty08l+XQbnk7ygSRfBd6fZK8kX2jbPBn8IrUkLbdt7jkmeSjwauCRVbU5yZ7A7w3N8vGqOrnN\n",
       "+1rgJcA7gROAJ1bVd5Ps1uY9HnhbVX04yYah+gqgqo5IcmNVHdzW9+QkB1XVRuBFwHtHlPrOqnpt\n",
       "W+5vkjytqj6zyOa+uqquT7IDcFaSA4HvAM+sqge3de82YvkA+1bVYUkeAJzdfvceDDymqm5O8nbg\n",
       "y1X1uiRPZfD83XHFQ2ENzFTVzCLbJm2TZPRRjYUel5Zbkilgapx5l+Kw6q8AH62qzQAtPIYfPzDJ\n",
       "64DdGfT6zmjTz2HQM/oocGqbdi7w6iT7AKdW1dULbPuvgRcl+UPgaOCwUXUm+WPgbsBewGXAYsPx\n",
       "mCTHMXje7gU8BLgC+EmS97T1jVpnAR8FqKqrk3yLQRD285xWVTe38cOBZ7VlTk9y/ZwrrppeZFuk\n",
       "JdVfHq4Pw9nHDUlNSus0zMyOJzlxvnmX4rBqMfehvtk/gFOA366qhwGvAXZqRb4U+B/AfYGLkuxV\n",
       "VR8Bng78GDg9yeMW2PbHgacATwMurKo5gyPJjsC7gCNbHScDO47dwsE69gdeDvxKVR0EfBbYqapu\n",
       "Ax4OfKzVccb8a5nTljmm/Ue/+UWuU5K0DZYiHP8eOGr2DMqhMylnd+i7AN9Lcmfg+bMLJTmgqs6v\n",
       "qhOB7wP7tAC6tqreAXwKOHDUhlvv6vPAScD7Rsw6G4Q/SLILcNRiGtjsBtwE/DDJPRmEciXZGdij\n",
       "qj4H/CFw0Ih1hMFzlfZ56v2Bq+aYZ9iXgecBJHkKsOdW1C5JWoRtPqxaVVckeT3wpSS3ARcD13J7\n",
       "z/EE4DwGAXgeg7AE+PMkD2QQBmdV1aVJXgkcm+RW4LvA62c3M7zJroQPMzjs+IURNd7QTma5DPhe\n",
       "q2Ox7dyY5GLgSuA64KvtoV2BT7XeaYA/GLUa4J+B8xmE7fFVdUv7UvR8Z8y+BvhIkucCXwP+abG1\n",
       "S5IWZ83fsirJHwG7th7odineskoTNtd7cL5bVvl+1Wox6r24XN9zXBFJPgHsz+CkIEmSlsSa7zn2\n",
       "kpzKIDCHvaKqzlzEOr4O3LWb/PyqunzM5V/FHT/X/GhVvXHcGhbD/8S1GnmzY612o/ad6y4ct0eG\n",
       "o1Yjw1Gr3ah9pxcelySpYzhKktQxHCVJ6hiOkpbN0KXi5rx6lbRaremvckhavW4/Aae8mKrWHHuO\n",
       "kiR1DEdJkjqGoyRJHcNRkqSO4ShJUsdw1JqUZGrSNaw027x9sM2rg+GotWpq0gVMwNSkC5iAqUkX\n",
       "MAFTky5gAqYmXUDPcJQkqWM4SpLU8ZZV60ASX0RJ2grez1GSpDF5WFWSpI7hKElSx3BcQ5I8OcmV\n",
       "Sf4xySvnmeft7fGNSQ5e6RqX2kJtTvJrra2XJjknycMmUedSGud1bvMdluSnSZ69kvUthzHf21NJ\n",
       "Lk5yWZKZFS5xyY3x3t47yRlJLmltfuEEylwySd6b5F+TbBoxz+rZf1WVP2vgB9gBuBrYD7gzcAnw\n",
       "kG6epwKnt+FfAr4+6bpXoM2PBHZvw0/eHto8NN/fA58Bjpx03SvwOu8BXA7s08b3nnTdK9DmaeCN\n",
       "s+0FfgBsmHTt29Dmw4GDgU3zPL6q9l/2HNeOhwNXV9W1VXUr8LfAM7p5jgDeD1BV5wF7JLnnypa5\n",
       "pBZsc1WdW1X/3kbPA/ZZ4RqX2jivM8DvAh8Dvr+SxS2Tcdr8PODjVfVtgKr6txWucamN0+bvAru1\n",
       "4d2AH1TVT1ewxiVVVV9h9E2vV9X+y3BcO+4DXDc0/u02baF51nJYjNPmYS8BTl/Wipbfgm1Och8G\n",
       "O9KT2qS1fsr5OK/zA4G9kpyd5MIkx65YdctjnDafDDw0yXeAjcDLVqi2SVlV+68Nk9qwFm3cHWD/\n",
       "nZ21vOMcu/YkjwNeDDx6+cpZEeO0+a3An1RVJQl3fM3XmnHafGfgEODxwN2Ac5N8var+cVkrWz7j\n",
       "tPlVwCVVNZXkAODMJAdV1Y3LXNskrZr9l+G4dvwLcN+h8fsy+M9q1Dz7tGlr1Thtpp2EczLw5Koa\n",
       "ddhmLRinzYcCfzvIRfYGnpLk1qo6bWVKXHLjtPk64N+q6sfAj5N8GTgIWKvhOE6bHwW8HqCq/m+S\n",
       "a4AHAReuSIUrb1XtvzysunZcCDwwyX5J7gIcA/Q7w9OAXwdI8gjghqr615Utc0kt2OYk9wNOBZ5f\n",
       "VVdPoMaltmCbq+r+VbV/Ve3P4HPHl67hYITx3tufAh6TZIckd2NwwsYVK1znUhqnzVcCTwBon709\n",
       "CPjWila5slbV/sue4xpRVT9N8jvA5xmc6faeqvqHJMe3x99dVacneWqSq4GbgBdNsORtNk6bgf8J\n",
       "7Amc1HpSt1bVwydV87Yas83rypjv7SuTnAFcCmwBTq6qNRuOY77ObwDel2Qjg47MK6pq88SK3kZJ\n",
       "PgI8Ftg7yXXAiQwOl6/K/ZeXj5MkqeNhVUmSOoajJEkdw1GSpI7hKElSx3CUJKljOEqS1DEcJW2z\n",
       "JD9a4PGpJJ/upp2S5MjlrUzaOoajpKWwNV+Yrq1cTlp2hqOkJZOBNyfZ1G5AffRCi6xIYdIiefk4\n",
       "SUvp2QwuCP4w4B7ABe0i4dKaYs9R0lJ6DPDhGvh/wJeAwxhcD3UuHlbVqmQ4SlpKxdz35PsBgwvE\n",
       "D9sL+P5KFCUtluEoaSl9BTgmyZ2S3AP4ZeB84Grg3kkeDJBkXwaHXy+ZWKXSCH7mKGkpFEBVfSLJ\n",
       "I4GNbdoft8OrJHk+g1sw7QjcCrxknd/VXmuYt6ySJKnjYVVJkjqGoyRJHcNRkqSO4ShJUsdwlCSp\n",
       "YzhKktQxHCVJ6hiOkiR1/j+PCeM0t7fOUgAAAABJRU5ErkJggg==\n"
      ],
      "text/plain": [
       "<matplotlib.figure.Figure at 0x111f5a050>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import sklearn.metrics\n",
    "\n",
    "def unweighted_accuracy(Y_pred, Y_ground_truth):\n",
    "    \"\"\"\n",
    "    This is simply the fraction of the test data which have been correctly labelled\n",
    "    \"\"\"\n",
    "    return sklearn.metrics.accuracy_score(Y_pred, Y_ground_truth)\n",
    "\n",
    "def class_weighted_accuracy(Y_pred, Y_ground_truth):\n",
    "    \"\"\"\n",
    "    This is the average fraction of each class in the ground truth examples which were correctly labelled.\n",
    "    This helps to remove bias caused by large imbalance in class labels in the testing set\n",
    "    \"\"\"\n",
    "#     all_labels_in_ground_truth = list(set(Y_ground_truth))\n",
    "    correct_label_count = defaultdict(int)\n",
    "    total_label_count = defaultdict(int)\n",
    "    \n",
    "    for y_pred, y_ground_truth in zip(Y_pred, Y_ground_truth):\n",
    "        \n",
    "        total_label_count[y_ground_truth] += 1\n",
    "        \n",
    "#         print y_pred, y_ground_truth\n",
    "        if y_pred == y_ground_truth:\n",
    "            correct_label_count[y_ground_truth] += 1\n",
    "        \n",
    "#     print correct_label_count\n",
    "    these_accs = []\n",
    "    for class_name, total_count in total_label_count.iteritems():\n",
    "        correct_count = correct_label_count[class_name]\n",
    "        these_accs.append(float(correct_count) / float(total_count))\n",
    "#         print class_name, correct_count, total_count\n",
    "    \n",
    "    return np.array(these_accs).mean()\n",
    "\n",
    "accuracies = []\n",
    "\n",
    "# Find the accuracy of each feature type\n",
    "for experiment_name, result_list in results.iteritems():\n",
    "    \n",
    "    # get a list of all the accuracy results for each train/test split\n",
    "    unweighted_acc = [unweighted_accuracy( result['Y_pred'], result['Y_test']) \n",
    "           for result in result_list]\n",
    "\n",
    "    weighted_acc = [class_weighted_accuracy( result['Y_pred'], result['Y_test']) \n",
    "           for result in result_list]\n",
    "\n",
    "    accuracies.append((experiment_name, unweighted_acc, weighted_acc))\n",
    "    \n",
    "# sorting the list of accuracies by median accuracy\n",
    "accuracies = sorted(accuracies, key=lambda x: np.median(np.array(x[1])))\n",
    "\n",
    "# printing this sorted list\n",
    "for experiment_name, acc1, acc2 in accuracies[::-1]:\n",
    "    print experiment_name.ljust(25) + '%0.3f\\t %0.3f' % (np.array(acc1).mean(), np.array(acc2).mean())\n",
    "\n",
    "plt.boxplot([acc[1] for acc in accuracies], 0, 'b+', 0);\n",
    "plt.xlim(0, 1.1)\n",
    "plt.xlabel('IoU')\n",
    "plt.gca().set_yticklabels([acc[0] for acc in accuracies]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Confusion matrix\n",
    "Only plotting for one run of the results.\n",
    "\n",
    "- Experiment type: `all`, i.e. with all the features used\n",
    "- Zeroth train/test split\n",
    "\n",
    "i.e. `results['all'][0]['Y_pred']`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "unique_labels = list(set(Y))\n",
    "\n",
    "cm = confusion_matrix(list(results['all'][0]['Y_pred']), results['all'][0]['Y_test'], labels=unique_labels)\n",
    "cm_normalized = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "\n",
    "def plot_confusion_matrix(cm, title='Confusion matrix', cmap=plt.cm.Blues):\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "#     plt.colorbar()\n",
    "    tick_marks = np.arange(len(unique_labels))\n",
    "    plt.xticks(tick_marks, unique_labels, rotation=45)\n",
    "    plt.yticks(tick_marks, unique_labels)\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "\n",
    "plt.figure(figsize=(10, 10))\n",
    "plot_confusion_matrix(cm);\n",
    "print unique_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature importances from the random forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# plotting feature importances for each \n",
    "plt.figure(figsize=(10, 3))\n",
    "plt.plot(rf.feature_importances_, '.')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
