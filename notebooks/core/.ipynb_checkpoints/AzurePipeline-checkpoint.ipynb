{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import inspect\n",
    "import hashlib\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def merge_two_dicts(x, y):\n",
    "    '''Given two dicts, merge them into a new dict as a shallow copy.\n",
    "    from http://stackoverflow.com/a/26853961/2156909'''\n",
    "    z = x.copy()\n",
    "    z.update(y)\n",
    "    return z\n",
    "\n",
    "class AzurePipeline(object):\n",
    "    def __init__(self, meta, cache_dir='.', save_intermediate_results=True):\n",
    "        self.meta = pd.DataFrame(meta)\n",
    "        self.df = None\n",
    "        \n",
    "        self.func_hash_dir = os.path.join(os.path.realpath(cache_dir),\n",
    "                                          'func_hash')\n",
    "        self.df_hash_dir = os.path.join(os.path.realpath(cache_dir),\n",
    "                                        'df_hash')\n",
    "        self.df_cache_dir = os.path.join(os.path.realpath(cache_dir),\n",
    "                                         'df_cache')\n",
    "        \n",
    "        self.save_intermediate_results = save_intermediate_results        \n",
    "        self.func_history = []\n",
    "        \n",
    "    \n",
    "    def __add__(self, other):        \n",
    "        d = merge_two_dicts(self.meta.to_dict(), other.meta.to_dict())\n",
    "        self.meta = pd.DataFrame(d)\n",
    "        \n",
    "        if other.df is not None:\n",
    "            if self.df is None:\n",
    "                self.df = other.df\n",
    "            else:\n",
    "                self.df = pd.concat((self.df, other.df))\n",
    "        \n",
    "        self.func_history = [self.func_history,\n",
    "                             other.func_history]\n",
    "        \n",
    "        return self\n",
    "        \n",
    "        \n",
    "    def get_df_hash_file(self, func):\n",
    "        try:\n",
    "            args = self.meta[func.__name__].dropna().to_dict()\n",
    "        except KeyError:\n",
    "            args = ''\n",
    "            \n",
    "        filename = os.path.join(self.df_hash_dir, \n",
    "                            hashlib.sha256('-'.join(self.func_history + [func.__name__]) \n",
    "                                            + '-' + str(args)).hexdigest()\n",
    "                            + '.sha256')\n",
    "        \n",
    "        if not os.path.exists(self.df_hash_dir):\n",
    "            os.makedirs(self.df_hash_dir)\n",
    "            \n",
    "        return filename\n",
    "        \n",
    "    \n",
    "    def get_func_hash_file(self, func):\n",
    "        filename = os.path.join(self.func_hash_dir, func.__name__ + '.sha256')\n",
    "    \n",
    "        if not os.path.exists(self.func_hash_dir):\n",
    "            os.makedirs(self.func_hash_dir)\n",
    "            \n",
    "        return filename\n",
    "        \n",
    "        \n",
    "    def get_df_cache_file(self, func):\n",
    "        try:\n",
    "            args = self.meta[func.__name__].dropna().to_dict()\n",
    "        except KeyError:\n",
    "            args = ''\n",
    "            \n",
    "        filename = os.path.join(self.df_cache_dir, \n",
    "                            hashlib.sha256('-'.join(self.func_history + [func.__name__]) \n",
    "                                            + '-' + str(args)).hexdigest()\n",
    "                            + '.pkl')   \n",
    "    \n",
    "        if not os.path.exists(self.df_cache_dir):\n",
    "            os.makedirs(self.df_cache_dir)\n",
    "            \n",
    "        return filename\n",
    "    \n",
    "    \n",
    "    def is_data_unchanged(self, func):\n",
    "        local_hash = hashlib.sha256(np.asarray(self.df).tostring()).hexdigest()\n",
    "        \n",
    "        filename = self.get_df_hash_file(func)\n",
    "        \n",
    "        if os.path.exists(filename):\n",
    "            with open(filename, 'r') as f:\n",
    "                saved_hash = f.readline()\n",
    "        else:\n",
    "            saved_hash = None\n",
    "            \n",
    "        if local_hash == saved_hash:\n",
    "            return True\n",
    "        else:\n",
    "            if self.save_intermediate_results:\n",
    "                with open(filename, 'w') as f:\n",
    "                    f.write(local_hash)\n",
    "            \n",
    "            return False           \n",
    "    \n",
    "    def is_func_unchanged(self, func):\n",
    "        source = inspect.getsource(func)\n",
    "        local_hash = hashlib.sha256(source).hexdigest()\n",
    "        \n",
    "        filename = self.get_func_hash_file(func)\n",
    "        \n",
    "        if os.path.exists(filename):\n",
    "            with open(filename, 'r') as f:\n",
    "                saved_hash = f.readline()\n",
    "        else:\n",
    "            saved_hash = None\n",
    "            \n",
    "        if local_hash == saved_hash:\n",
    "            return True\n",
    "        else:\n",
    "            if self.save_intermediate_results:\n",
    "                with open(filename, 'w') as f:\n",
    "                    f.write(local_hash)\n",
    "            \n",
    "            return False           \n",
    "        \n",
    "            \n",
    "    def cache(self, func):\n",
    "        filename = self.get_df_cache_file(func)\n",
    "        self.df.to_pickle(filename)\n",
    "    \n",
    "    \n",
    "    def load_from_cache(self, func):\n",
    "        filename = self.get_df_cache_file(func)\n",
    "        self.df = pd.read_pickle(filename)\n",
    "        \n",
    "        \n",
    "    def apply(self, func, args=None):\n",
    "        if args:\n",
    "            d = merge_two_dicts(self.meta.to_dict(), args)\n",
    "            self.meta = pd.DataFrame(d)\n",
    "        \n",
    "        if not self.is_data_unchanged(func) or \\\n",
    "        not self.is_func_unchanged(func):\n",
    "            self.df, self.meta = func(self.df, self.meta,)        \n",
    "            self.cache(func)\n",
    "        else:\n",
    "            self.load_from_cache(func)\n",
    "            \n",
    "        self.func_history += [func.__name__]\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def read_wav(df, meta):\n",
    "    args = meta['read_wav']\n",
    "    \n",
    "    print \"read_wav\"\n",
    "    import scipy.io.wavfile\n",
    "    sound = scipy.io.wavfile.read(args['in_filename'])\n",
    "    df = pd.DataFrame(sound[1])\n",
    "    return df, meta\n",
    "    \n",
    "def test2(df, meta):\n",
    "#     args = meta['test2']\n",
    "    \n",
    "    print \"test2\"\n",
    "    df /= 3\n",
    "    return df, meta\n",
    "    \n",
    "def save(df, meta):\n",
    "    args = meta['save']\n",
    "    \n",
    "    print \"save\"\n",
    "    \n",
    "    df.to_csv(args['out_filename'])\n",
    "    \n",
    "    return df, meta\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test2\n",
      "save\n"
     ]
    }
   ],
   "source": [
    "meta = {'read_wav': {'in_filename': '../../../data/night.wav'},\n",
    "        'save': {'out_filename': 'night.csv'}}\n",
    "\n",
    "ap = AzurePipeline(meta)\n",
    "\n",
    "ap.apply(read_wav)\n",
    "ap.apply(test2)\n",
    "ap.apply(save)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> \u001b[0;32m<ipython-input-127-0861ecf8b204>\u001b[0m(88)\u001b[0;36mis_data_unchanged\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m     87 \u001b[0;31m            \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_intermediate_results\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m---> 88 \u001b[0;31m                \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'w'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m     89 \u001b[0;31m                    \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlocal_hash\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\n",
      "ipdb> self.func_hash_dir\n",
      "'/Users/peter/Documents/phd/projects/azure/engaged_hackathon/core/notebooks/func_hash'\n",
      "ipdb> os.path.exists(self.func_hash_dir)\n",
      "False\n",
      "ipdb> os.makedirs(self.func_hash_dir)\n",
      "ipdb> q\n"
     ]
    }
   ],
   "source": [
    "debug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "a = pd.DataFrame({'read_wav': {'in_filename': '../../../data/day.wav'},\n",
    "              'save': {'out_filename': 'test_night.csv'}})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'in_filename': '../../../data/day.wav'}"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a['read_wav'].dropna().to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>read_wav</th>\n",
       "      <th>save</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>in_filename</th>\n",
       "      <td>../../../data/day.wav</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>out_filename</th>\n",
       "      <td>NaN</td>\n",
       "      <td>test_night.csv</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           read_wav            save\n",
       "in_filename   ../../../data/day.wav             NaN\n",
       "out_filename                    NaN  test_night.csv"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(meta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'' == None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
