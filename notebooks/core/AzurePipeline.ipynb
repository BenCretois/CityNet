{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import inspect\n",
    "import hashlib\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def merge_two_dicts(x, y):\n",
    "    '''Given two dicts, merge them into a new dict as a shallow copy.\n",
    "    from http://stackoverflow.com/a/26853961/2156909'''\n",
    "    z = x.copy()\n",
    "    z.update(y)\n",
    "    return z\n",
    "\n",
    "class AzurePipeline(object):\n",
    "    \"\"\" Class to mimic Azure behaviour \"\"\"\n",
    "    \n",
    "    def __init__(self, meta, cache_dir='.', save_intermediate_results=True):\n",
    "        \"\"\"\n",
    "        meta (dict(dict)):\n",
    "                {function-name1: {arg1: value1,\n",
    "                                  arg2: value2},\n",
    "                 function-name2: {arg1: value1}}\n",
    "                 \n",
    "        cache_dir (string):\n",
    "                folder for caching data\n",
    "                \n",
    "        save_intermediate_results (bool):\n",
    "                save (cache) intermediate results [yes/no]\n",
    "        \"\"\"\n",
    "        \n",
    "        self.meta = pd.DataFrame(meta)\n",
    "        self.df = None\n",
    "        \n",
    "        self.func_hash_dir = os.path.join(os.path.realpath(cache_dir),\n",
    "                                          'func_hash')\n",
    "        self.df_hash_dir = os.path.join(os.path.realpath(cache_dir),\n",
    "                                        'df_hash')\n",
    "        self.df_cache_dir = os.path.join(os.path.realpath(cache_dir),\n",
    "                                         'df_cache')\n",
    "        \n",
    "        self.save_intermediate_results = save_intermediate_results        \n",
    "        self.func_history = []\n",
    "        \n",
    "    \n",
    "    def __add__(self, other):  \n",
    "        \"\"\" overloaded + operator. Returns a merged AzurePipeline object\"\"\"\n",
    "        d = merge_two_dicts(self.meta.to_dict(), other.meta.to_dict())\n",
    "        \n",
    "        ap = AzurePipeline(d)\n",
    "        \n",
    "        if other.df is not None:\n",
    "            if self.df is None:\n",
    "                ap.df = other.df\n",
    "            else:\n",
    "                ap.df = pd.concat((self.df, other.df), copy=True)\n",
    "        \n",
    "        ap.func_history = [self.func_history,\n",
    "                             other.func_history]\n",
    "        \n",
    "        return ap\n",
    "        \n",
    "        \n",
    "    def get_df_hash_file(self, func):\n",
    "        \"\"\" get filename of DataFrame hash \"\"\"\n",
    "        try:\n",
    "            args = self.meta[func.__name__].dropna().to_dict()\n",
    "        except KeyError:\n",
    "            args = ''\n",
    "            \n",
    "        filename = os.path.join(self.df_hash_dir, \n",
    "                            hashlib.sha256('-'.join(self.func_history + [func.__name__]) \n",
    "                                            + '-' + str(args)).hexdigest()\n",
    "                            + '.sha256')\n",
    "        \n",
    "        if not os.path.exists(self.df_hash_dir):\n",
    "            os.makedirs(self.df_hash_dir)\n",
    "            \n",
    "        return filename\n",
    "        \n",
    "    \n",
    "    def get_func_hash_file(self, func):\n",
    "        \"\"\" get filename of function source hash \"\"\"\n",
    "        filename = os.path.join(self.func_hash_dir, func.__name__ + '.sha256')\n",
    "    \n",
    "        if not os.path.exists(self.func_hash_dir):\n",
    "            os.makedirs(self.func_hash_dir)\n",
    "            \n",
    "        return filename\n",
    "        \n",
    "        \n",
    "    def get_df_cache_file(self, func):\n",
    "        \"\"\" get filename of DataFrame cache (data) \"\"\"\n",
    "        try:\n",
    "            args = self.meta[func.__name__].dropna().to_dict()\n",
    "        except KeyError:\n",
    "            args = ''\n",
    "            \n",
    "        filename = os.path.join(self.df_cache_dir, \n",
    "                            hashlib.sha256('-'.join(self.func_history + [func.__name__]) \n",
    "                                            + '-' + str(args)).hexdigest()\n",
    "                            + '.pkl')   \n",
    "    \n",
    "        if not os.path.exists(self.df_cache_dir):\n",
    "            os.makedirs(self.df_cache_dir)\n",
    "            \n",
    "        return filename\n",
    "    \n",
    "    \n",
    "    def is_data_unchanged(self, func):\n",
    "        \"\"\" check whether the data has already been used as argument of func \"\"\"\n",
    "        local_hash = hashlib.sha256(np.asarray(self.df).tostring()).hexdigest()\n",
    "        \n",
    "        filename = self.get_df_hash_file(func)\n",
    "        \n",
    "        if os.path.exists(filename):\n",
    "            with open(filename, 'r') as f:\n",
    "                saved_hash = f.readline()\n",
    "        else:\n",
    "            saved_hash = None\n",
    "            \n",
    "        if local_hash == saved_hash:\n",
    "            return True\n",
    "        else:\n",
    "            if self.save_intermediate_results:\n",
    "                with open(filename, 'w') as f:\n",
    "                    f.write(local_hash)\n",
    "            \n",
    "            return False           \n",
    "    \n",
    "    def is_func_unchanged(self, func):\n",
    "        \"\"\" check whether the source code of func has remained stable \"\"\"\n",
    "        source = inspect.getsource(func)\n",
    "        local_hash = hashlib.sha256(source).hexdigest()\n",
    "        \n",
    "        filename = self.get_func_hash_file(func)\n",
    "        \n",
    "        if os.path.exists(filename):\n",
    "            with open(filename, 'r') as f:\n",
    "                saved_hash = f.readline()\n",
    "        else:\n",
    "            saved_hash = None\n",
    "            \n",
    "        if local_hash == saved_hash:\n",
    "            return True\n",
    "        else:\n",
    "            if self.save_intermediate_results:\n",
    "                with open(filename, 'w') as f:\n",
    "                    f.write(local_hash)\n",
    "            \n",
    "            return False           \n",
    "        \n",
    "            \n",
    "    def cache(self, func):\n",
    "        \"\"\" dump DataFrame out to cache \"\"\"\n",
    "        filename = self.get_df_cache_file(func)\n",
    "        self.df.to_pickle(filename)\n",
    "    \n",
    "    \n",
    "    def load_from_cache(self, func):\n",
    "        \"\"\" load previous computation from cache \"\"\"\n",
    "        filename = self.get_df_cache_file(func)\n",
    "        self.df = pd.read_pickle(filename)\n",
    "        \n",
    "        \n",
    "    def apply(self, func, args=None):\n",
    "        \"\"\" apply func to pipeline, using complementary args if necessary \"\"\"\n",
    "        if args:\n",
    "            d = merge_two_dicts(self.meta.to_dict(), args)\n",
    "            self.meta = pd.DataFrame(d)\n",
    "\n",
    "        # evaluate outside of condition to make sure\n",
    "        # side effects are taking place\n",
    "        data_unchanged = self.is_data_unchanged(func)\n",
    "        func_unchanged = self.is_func_unchanged(func)\n",
    "        \n",
    "        if not data_unchanged or not func_unchanged:\n",
    "            self.df, self.meta = func(self.df, self.meta,)        \n",
    "            self.cache(func)\n",
    "        else:\n",
    "            self.load_from_cache(func)\n",
    "            \n",
    "        self.func_history += [func.__name__]\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def read_wav(df, meta):\n",
    "    args = meta['read_wav']\n",
    "    \n",
    "    print \"read_wav\"\n",
    "    import scipy.io.wavfile\n",
    "    sound = scipy.io.wavfile.read(args['in_filename'])\n",
    "    df = pd.DataFrame(sound[1])\n",
    "    return df, meta\n",
    "    \n",
    "def test2(df, meta):\n",
    "#     args = meta['test2']\n",
    "    \n",
    "    print \"test2\"\n",
    "    df /= 3\n",
    "    return df, meta\n",
    "    \n",
    "def save(df, meta):\n",
    "    args = meta['save']\n",
    "    \n",
    "    print \"save\"\n",
    "    \n",
    "    df.to_csv(args['out_filename'])\n",
    "    \n",
    "    return df, meta\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from \n",
    "\n",
    "meta = {'read_wav': {'in_filename': '../../../data/night.wav'},\n",
    "        'save': {'out_filename': 'night.csv'}}\n",
    "\n",
    "ap = AzurePipeline(meta)\n",
    "\n",
    "ap.apply(read_wav)\n",
    "ap.apply(test2)\n",
    "ap.apply(save)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "concat\n"
     ]
    }
   ],
   "source": [
    "ap2 = ap + ap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1279488"
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(ap.df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2558976"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(ap2.df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>read_wav</th>\n",
       "      <th>save</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>in_filename</th>\n",
       "      <td>../../../data/day.wav</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>out_filename</th>\n",
       "      <td>NaN</td>\n",
       "      <td>test_night.csv</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           read_wav            save\n",
       "in_filename   ../../../data/day.wav             NaN\n",
       "out_filename                    NaN  test_night.csv"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(meta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'' == None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
