{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extracting training and testing patches from the spectrograms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import csv\n",
    "import os\n",
    "import numpy as np\n",
    "import collections\n",
    "\n",
    "import scipy.io.wavfile\n",
    "from scipy.io import loadmat\n",
    "from sklearn.cross_validation import train_test_split\n",
    "import collections\n",
    "\n",
    "import sys, os\n",
    "sys.path.append(os.path.expanduser('~/projects/engaged_hackathon/'))\n",
    "from engaged.features import frequency\n",
    "from scipy.ndimage.interpolation import zoom\n",
    "from time import time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "362\n"
     ]
    }
   ],
   "source": [
    "# getting a list of all the files\n",
    "base_path = '/home/michael/projects/engaged_hackathon_data/raw_data/one_minute_files'\n",
    "files = os.listdir(base_path + '/detection_challenge')\n",
    "files = [xx.split('.')[0] for xx in files if 'sampled' not in xx]\n",
    "print len(files)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## For each file, extract 1D patches at random locations\n",
    "\n",
    "Ensure that the same number of positive and negative patches are extracted from each file!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "362\n",
      "0 5 10 15 20 25 30 35 40 45 50 55 60 65 70 75 80 85 90 95 100 105 110 115 120 125 130 135 140 145 150 155 160 165 170 175 180 185 190 195 200 205 210 215 220 225 230 235 240 245 250 255 260 265 270 275 280 285 290 295 300 305 310 315 320 325 330 335 340 345 350 355 360\n"
     ]
    }
   ],
   "source": [
    "def choose_locations(idxs, maximum, balance=False):\n",
    "    \"\"\"\n",
    "    Given a binary array, the function returns a list of positive and negative locations sampled\n",
    "    at random from the list.\n",
    "    Returns 'maximum' locations unless there there are fewer than maximum locations in\n",
    "    idxs, in which case all are returned\n",
    "    If balance is true then the classes are balanced to the smaller class size to\n",
    "    ensure an equal number of each\n",
    "    \"\"\"\n",
    "    false_idxs = np.where(idxs==0)[0]\n",
    "    true_idxs = np.where(idxs==1)[0]\n",
    "    \n",
    "    if false_idxs.shape[0] > maximum and maximum is not None:\n",
    "        false_idxs = np.random.choice(false_idxs, maximum, replace=False)\n",
    "        \n",
    "    if true_idxs.shape[0] > maximum and maximum is not None:\n",
    "        true_idxs = np.random.choice(true_idxs, maximum, replace=False)\n",
    "        \n",
    "    if false_idxs.shape[0] != true_idxs.shape[0] and balance:\n",
    "        raise Exception(\"Not implemented!\")\n",
    "        \n",
    "    return true_idxs, false_idxs\n",
    "    \n",
    "\n",
    "def extract_1d_patches(array, locations, hww):\n",
    "    \"\"\"\n",
    "    Extract vertical patches from the array, at the locations given.\n",
    "    Each slice has a half window width hww\n",
    "    \n",
    "    Returns an array of shape:\n",
    "    (len(locations), array.shape[0], hww*2+1)\n",
    "    \"\"\"\n",
    "    # pad the array to account for overspill\n",
    "    offset_idxs_np = np.array(locations) + hww\n",
    "    extra1 = np.tile(array[:, 0], (hww, 1)).T\n",
    "    extra2 = np.tile(array[:, -1], (hww, 1)).T\n",
    "    a_temp = np.hstack((extra1, array, extra2))\n",
    "    \n",
    "    # set up the array of index locations to extract from\n",
    "    idxs = [offset_idxs_np]\n",
    "    for offset in range(1, hww+1):\n",
    "        idxs.insert(0, offset_idxs_np-offset)\n",
    "        idxs.append(offset_idxs_np+offset)\n",
    "    new_idx = np.vstack(idxs).T.ravel()\n",
    "    \n",
    "    # extract the patches and do the appropriate reshapgin\n",
    "    new_shape = (array.shape[0], offset_idxs_np.shape[0], hww*2 + 1)\n",
    "    to_return = a_temp[:, new_idx].reshape(new_shape).transpose((1, 0, 2))\n",
    "    return to_return\n",
    "\n",
    "\n",
    "max_from_each_file = 500\n",
    "spec_path = '/home/michael/projects/engaged_hackathon_data/detection/spectrograms/'\n",
    "\n",
    "\n",
    "def process_file(inputs):\n",
    "    \n",
    "    count, fname = inputs\n",
    "    \n",
    "    # load in wav and convert to spectrogram\n",
    "    spec = scipy.io.loadmat(spec_path + fname + '_smallspec.mat')['spectrogram']\n",
    "        \n",
    "    # load in ground truth\n",
    "    gt = loadmat(base_path + '/detection_challenge/' + fname + '.mat')\n",
    "    \n",
    "    # convert the labels to the sampling rate of the spectrogram\n",
    "    zoom_factor = float(spec.shape[1]) / float(gt['biotic'][0].shape[0])\n",
    "    gt_biotic = zoom(gt['biotic'][0], zoom_factor).astype(bool)\n",
    "    gt_anthrop = zoom(gt['anthropogenic'][0], zoom_factor).astype(bool)\n",
    "    \n",
    "    # choosing where to extract from..\n",
    "    tic = time()    \n",
    "    true_idxs, false_idxs = choose_locations(gt_biotic, max_from_each_file)\n",
    "    idxs = np.hstack([true_idxs, false_idxs])\n",
    "    labels = np.hstack(\n",
    "        [np.ones(true_idxs.shape), np.zeros(false_idxs.shape)])\n",
    "    \n",
    "    # let's group the X here and create a suitable Y vector...\n",
    "    patches = extract_1d_patches(spec, idxs, hww=9)\n",
    "\n",
    "    if count % 5 == 0:\n",
    "        print count,\n",
    "    \n",
    "    return patches, labels\n",
    "\n",
    "print len(files)\n",
    "\n",
    "XY = map(process_file, enumerate(files))\n",
    "tX, tY = zip(*XY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train/test split and combining data\n",
    "\n",
    "The split is done at the file level. In the future, I should really do this at location level."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_test (18286, 1, 75, 19) float32 0.170399\n",
      "X_train (50836, 1, 75, 19) float32 0.175369\n",
      "y_train (50836,) int32 0.5\n",
      "y_test (18286,) int32 0.5\n"
     ]
    }
   ],
   "source": [
    "def balance_classes(X, Y):\n",
    "    \"\"\"\n",
    "    Returns X, Y, where there are equal numbers of Y==0 as Y==1.\n",
    "    \"\"\"\n",
    "    positives = np.where(Y == 0)[0]\n",
    "    negatives = np.where(Y == 1)[0]\n",
    "    max_examples = min(len(positives), len(negatives))\n",
    "    \n",
    "    if len(positives) > max_examples:\n",
    "        positives = np.random.choice(positives, max_examples, replace=False)\n",
    "    if len(negatives) > max_examples:\n",
    "        negatives = np.random.choice(negatives, max_examples, replace=False)\n",
    "        \n",
    "    new_X = np.vstack((X[negatives, :], X[positives, :]))\n",
    "    new_Y = np.hstack((Y[negatives], Y[positives])) \n",
    "    \n",
    "    # return the output in a random order\n",
    "    shuffle_idxs = np.random.permutation(new_X.shape[0])\n",
    "    \n",
    "    return new_X[shuffle_idxs, :], new_Y[shuffle_idxs]\n",
    "    \n",
    "\n",
    "# Doing the full train/test split\n",
    "train_files, test_files = train_test_split(\n",
    "    range(len(files)), random_state=0, train_size=0.7, test_size=0.3)\n",
    "\n",
    "# Extracting the data for each side of the split\n",
    "data = {}\n",
    "data['X_train'] = np.vstack([tX[idx] for idx in train_files])\n",
    "data['y_train'] = np.hstack([tY[idx] for idx in train_files]).astype(np.int32).ravel()\n",
    "data['X_test'] = np.vstack([tX[idx] for idx in test_files])\n",
    "data['y_test'] = np.hstack([tY[idx] for idx in test_files]).astype(np.int32).ravel()\n",
    "\n",
    "# Ensuring data is the correct shape\n",
    "for key in ['X_train', 'X_test']:\n",
    "    tshape = data[key].shape\n",
    "    data[key] = data[key].reshape((tshape[0], -1, tshape[1], tshape[2]))\n",
    "    data[key] = data[key].astype(np.float32)\n",
    "\n",
    "# balance the classes...\n",
    "for key in ['train', 'test']:\n",
    "    data['X_' + key], data['y_' + key] = balance_classes(data['X_' + key], data['y_' + key])\n",
    "    \n",
    "# Print details to screen\n",
    "for key, val in data.iteritems():\n",
    "    print key, val.shape, val.dtype, val.mean()\n",
    "\n",
    "# save full dataset to disk\n",
    "savedir = '/home/michael/projects/engaged_hackathon_data/detection/train_test_patches/'\n",
    "scipy.io.savemat(savedir + 'full.mat', data, do_compression=True, oned_as='column')\n",
    "\n",
    "# Also save which are the training and test files\n",
    "train_filenames = [files[idx] for idx in train_files]\n",
    "test_filenames = [files[idx] for idx in test_files]\n",
    "D = dict(train_files=train_filenames, test_files=test_filenames, train_file_idxs=train_files, test_file_idxs=test_files)\n",
    "scipy.io.savemat(savedir + 'split.mat', D)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a subset of the train/test dataset and save to disk "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_test (2000, 1, 75, 19)\n",
      "X_train (4000, 1, 75, 19)\n",
      "y_train (4000,)\n",
      "y_test (2000,)\n"
     ]
    }
   ],
   "source": [
    "# Take a subset of the full train/test split and save to disk\n",
    "maxi = {}\n",
    "maxi['train'] = 4000\n",
    "maxi['test'] = 2000\n",
    "\n",
    "small_data = {}\n",
    "\n",
    "for key in ['train', 'test']:\n",
    "    N = data['X_' + key].shape[0]\n",
    "    idxs = np.random.permutation(N)[:maxi[key]]\n",
    "    small_data['X_' + key] = data['X_' + key][idxs, :]\n",
    "    small_data['y_' + key] = data['y_' + key][idxs]\n",
    "\n",
    "#     data['X_' + key], data['y_' + key] = balance_classes(data['X_' + key], data['y_' + key])\n",
    "\n",
    "for key, val in small_data.iteritems():\n",
    "    print key, val.shape\n",
    "    \n",
    "scipy.io.savemat(savedir + 'small.mat', small_data, do_compression=True, oned_as='column')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
