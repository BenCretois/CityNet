{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extracting training and testing patches from the spectrograms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import csv\n",
    "import os\n",
    "import numpy as np\n",
    "import collections\n",
    "\n",
    "import scipy.io.wavfile\n",
    "from scipy.io import loadmat\n",
    "from sklearn.cross_validation import train_test_split\n",
    "import collections\n",
    "\n",
    "import sys, os\n",
    "sys.path.append(os.path.expanduser('~/projects/engaged_hackathon/'))\n",
    "from engaged.features import frequency\n",
    "from scipy.ndimage.interpolation import zoom\n",
    "from time import time\n",
    "import cPickle as pickle\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "362\n"
     ]
    }
   ],
   "source": [
    "# getting a list of all the files\n",
    "base_path = '/home/michael/projects/engaged_hackathon_data/raw_data/one_minute_files'\n",
    "files = os.listdir(base_path + '/detection_challenge')\n",
    "files = [xx.split('.')[0] for xx in files if 'sampled' not in xx]\n",
    "print len(files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['W112NN-13548_20130708_2146', 'W84LA-013548_20130627_1125']\n"
     ]
    }
   ],
   "source": [
    "# Doing the full train/test split\n",
    "savedir = '/home/michael/projects/engaged_hackathon_data/detection/train_test_patches/'\n",
    "\n",
    "train_files, test_files = train_test_split(\n",
    "    range(len(files)), random_state=0, train_size=0.7, test_size=0.3)\n",
    "\n",
    "# Also save which are the training and test files\n",
    "train_filenames = [files[idx] for idx in train_files]\n",
    "test_filenames = [files[idx] for idx in test_files]\n",
    "\n",
    "D = dict(train_files=train_filenames, test_files=test_filenames, \n",
    "         train_file_idxs=train_files, test_file_idxs=test_files)\n",
    "print D['train_files'][:2]\n",
    "scipy.io.savemat(savedir + 'split.mat', D)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## For each file, extract 1D patches at random locations\n",
    "\n",
    "Ensure that the same number of positive and negative patches are extracted from each file!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# setting options\n",
    "should_I_balance = False\n",
    "max_from_each_file = 2000\n",
    "savename = 'unbalanced_256'\n",
    "spec_name = '_spec_256.mat'\n",
    "should_I_subsample = True\n",
    "max_samples = {'train': 100000, 'test': 30000}\n",
    "# del tX, tY, XY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "362\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(10)\n",
    "\n",
    "def choose_locations(idxs, maximum, random_sample=True, balance=False):\n",
    "    \"\"\"\n",
    "    Given a binary array, the function returns a list of positive and negative locations sampled\n",
    "    at random from the list.\n",
    "    Returns 'maximum' locations unless there there are fewer than maximum locations in\n",
    "    idxs, in which case all are returned\n",
    "    If balance is true then the classes are balanced to the smaller class size to\n",
    "    ensure an equal number of each\n",
    "    \"\"\"\n",
    "    \n",
    "    if random_sample:\n",
    "        idxs = idxs.astype(int)\n",
    "        \n",
    "        to_ignore = np.random.choice(\n",
    "            idxs.shape[0], idxs.shape[0]-maximum, replace=False)\n",
    "        idxs[to_ignore] = 2\n",
    "        \n",
    "    false_idxs = np.where(idxs==0)[0]\n",
    "    true_idxs = np.where(idxs==1)[0]\n",
    "        \n",
    "    if false_idxs.shape[0] > maximum and maximum is not None and balance:\n",
    "        print \"Sholt\"\n",
    "        false_idxs = np.random.choice(false_idxs, maximum, replace=False)\n",
    "        \n",
    "    if true_idxs.shape[0] > maximum and maximum is not None and balance:\n",
    "        true_idxs = np.random.choice(true_idxs, maximum, replace=False)\n",
    "        \n",
    "    if false_idxs.shape[0] != true_idxs.shape[0] and balance:\n",
    "        raise Exception(\"Not implemented!\")\n",
    "        \n",
    "    return true_idxs, false_idxs\n",
    "    \n",
    "\n",
    "def extract_1d_patches(array, locations, hww):\n",
    "    \"\"\"\n",
    "    Extract vertical patches from the array, at the locations given.\n",
    "    Each slice has a half window width hww\n",
    "    \n",
    "    Returns an array of shape:\n",
    "    (len(locations), array.shape[0], hww*2+1)\n",
    "    \"\"\"\n",
    "    # pad the array to account for overspill\n",
    "    offset_idxs_np = np.array(locations) + hww\n",
    "    extra1 = np.tile(array[:, 0], (hww, 1)).T\n",
    "    extra2 = np.tile(array[:, -1], (hww, 1)).T\n",
    "    a_temp = np.hstack((extra1, array, extra2))\n",
    "    \n",
    "    # set up the array of index locations to extract from\n",
    "    idxs = [offset_idxs_np]\n",
    "    for offset in range(1, hww+1):\n",
    "        idxs.insert(0, offset_idxs_np-offset)\n",
    "        idxs.append(offset_idxs_np+offset)\n",
    "    new_idx = np.vstack(idxs).T.ravel()\n",
    "    \n",
    "    # extract the patches and do the appropriate reshapgin\n",
    "    new_shape = (array.shape[0], offset_idxs_np.shape[0], hww*2 + 1)\n",
    "    to_return = a_temp[:, new_idx].reshape(new_shape).transpose((1, 0, 2))\n",
    "    return to_return\n",
    "\n",
    "spec_path = '/home/michael/projects/engaged_hackathon_data/detection/spectrograms/'\n",
    "\n",
    "\n",
    "def process_file(inputs):\n",
    "    \n",
    "    count, fname = inputs\n",
    "    \n",
    "    # load in wav and convert to spectrogram\n",
    "    spec = scipy.io.loadmat(spec_path + fname + spec_name)['spectrogram']\n",
    "        \n",
    "    # load in ground truth\n",
    "    gt = loadmat(base_path + '/detection_challenge/' + fname + '.mat')\n",
    "    \n",
    "    # convert the labels to the sampling rate of the spectrogram\n",
    "    zoom_factor = float(spec.shape[1]) / float(gt['biotic'][0].shape[0])\n",
    "    gt_biotic = zoom(gt['biotic'][0], zoom_factor).astype(bool)\n",
    "    gt_anthrop = zoom(gt['anthropogenic'][0], zoom_factor).astype(bool)\n",
    "    \n",
    "    # choosing where to extract from..\n",
    "    tic = time()    \n",
    "    true_idxs, false_idxs = choose_locations(gt_biotic, max_from_each_file)\n",
    "    idxs = np.hstack([true_idxs, false_idxs])\n",
    "    labels = np.hstack(\n",
    "        [np.ones(true_idxs.shape), np.zeros(false_idxs.shape)]).astype(np.int32)\n",
    "    \n",
    "    # let's group the X here and create a suitable Y vector...\n",
    "    patches = extract_1d_patches(spec, idxs, hww=9).astype(np.float32)\n",
    "\n",
    "    if count % 5 == 0:\n",
    "        print count,\n",
    "    \n",
    "    return patches, labels\n",
    "\n",
    "print len(files)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 5 10 15 20 25 30 35 40 45 50 55 60 65 70 75 80 85 90 95 100 105 110 115 120 125 130 135 140 145 150 155 160 165 170 175 180 185 190 195 200 205 210 215 220 225 230 235 240 245 250\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/michael/anaconda/lib/python2.7/site-packages/scipy/ndimage/interpolation.py:549: UserWarning: From scipy 0.13.0, the output shape of zoom() is calculated with round() instead of int() - for these inputs the size of the returned array has changed.\n",
      "  \"the returned array has changed.\", UserWarning)\n"
     ]
    }
   ],
   "source": [
    "XY = map(process_file, enumerate(train_filenames))\n",
    "tX, tY = zip(*XY)\n",
    "\n",
    "del XY\n",
    "data = {}\n",
    "data['X_train'] = np.vstack(tX)\n",
    "data['y_train'] = np.hstack(tY).astype(np.int32).ravel()\n",
    "del tX, tY\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(506000, 1, 256, 19)\n",
      "(506000,)\n",
      "(100000, 1, 256, 19)\n",
      "(100000,)\n"
     ]
    }
   ],
   "source": [
    "print data['X_train'].shape\n",
    "print data['y_train'].shape\n",
    "\n",
    "if should_I_balance:\n",
    "    # balance the classes...\n",
    "    for key in ['train']:\n",
    "        data['X_' + key], data['y_' + key] = balance_classes(data['X_' + key], data['y_' + key])\n",
    "\n",
    "if should_I_subsample:\n",
    "    for key in ['train']:\n",
    "        data['X_' + key], data['y_' + key] = subsample_data(data['X_' + key], data['y_' + key], max_samples[key])\n",
    "\n",
    "print data['X_train'].shape\n",
    "print data['y_train'].shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100000\n"
     ]
    }
   ],
   "source": [
    "print max_samples['train']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 5 10 15 20 25 30 35 40 45 50 55 60 65 70 75 80 85 90 95 100 105\n"
     ]
    }
   ],
   "source": [
    "XY = map(process_file, enumerate(test_filenames))\n",
    "tX, tY = zip(*XY)\n",
    "data['X_test'] = np.vstack(tX)\n",
    "data['y_test'] = np.hstack(tY).astype(np.int32).ravel()\n",
    "del tX, tY\n",
    "\n",
    "\n",
    "if should_I_balance:\n",
    "    # balance the classes...\n",
    "    for key in ['test']:\n",
    "        data['X_' + key], data['y_' + key] = balance_classes(data['X_' + key], data['y_' + key])\n",
    "\n",
    "if should_I_subsample:\n",
    "    for key in ['test']:\n",
    "        data['X_' + key], data['y_' + key] = subsample_data(data['X_' + key], data['y_' + key], max_samples[key])\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# print np.sum([xx.shape[0] for xx in tX])\n",
    "# print np.sum([xx for xx in tY])\n",
    "# del XY"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train/test split and combining data\n",
    "\n",
    "The split is done at the file level. In the future, I should really do this at location level."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'X_test'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-13-18a1519f6bc3>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     36\u001b[0m \u001b[1;31m# Ensuring data is the correct shape\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     37\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mkey\u001b[0m \u001b[1;32min\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;34m'X_train'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'X_test'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 38\u001b[1;33m     \u001b[0mtshape\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     39\u001b[0m     \u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     40\u001b[0m     \u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'X_test'"
     ]
    }
   ],
   "source": [
    "def balance_classes(X, Y):\n",
    "    \"\"\"\n",
    "    Returns X, Y, where there are equal numbers of Y==0 as Y==1.\n",
    "    \"\"\"\n",
    "    positives = np.where(Y == 0)[0]\n",
    "    negatives = np.where(Y == 1)[0]\n",
    "    max_examples = min(len(positives), len(negatives))\n",
    "    \n",
    "    if len(positives) > max_examples:\n",
    "        positives = np.random.choice(positives, max_examples, replace=False)\n",
    "    if len(negatives) > max_examples:\n",
    "        negatives = np.random.choice(negatives, max_examples, replace=False)\n",
    "        \n",
    "    new_X = np.vstack((X[negatives, :], X[positives, :]))\n",
    "    new_Y = np.hstack((Y[negatives], Y[positives])) \n",
    "    \n",
    "    # return the output in a random order\n",
    "    shuffle_idxs = np.random.permutation(new_X.shape[0])\n",
    "    \n",
    "    return new_X[shuffle_idxs, :], new_Y[shuffle_idxs]\n",
    "\n",
    "def subsample_data(X, Y, num):\n",
    "    if num > X.shape[0]:\n",
    "        return X, Y\n",
    "    else:\n",
    "        to_use = np.random.choice(X.shape[0], num, replace=False)\n",
    "        return X[to_use, :], Y[to_use]\n",
    "\n",
    "# Extracting the data for each side of the split\n",
    "# data = {}\n",
    "# data['X_train'] = np.vstack([tX[idx] for idx in train_files])\n",
    "# data['y_train'] = np.hstack([tY[idx] for idx in train_files]).astype(np.int32).ravel()\n",
    "# data['X_test'] = np.vstack([tX[idx] for idx in test_files])\n",
    "# data['y_test'] = np.hstack([tY[idx] for idx in test_files]).astype(np.int32).ravel()\n",
    "\n",
    "# Ensuring data is the correct shape\n",
    "for key in ['X_train', 'X_test']:\n",
    "    tshape = data[key].shape\n",
    "    data[key] = data[key].reshape((tshape[0], -1, tshape[1], tshape[2]))\n",
    "    data[key] = data[key].astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_test (30000, 256, 19) float32 0.158177\n",
      "X_train (100000, 1, 256, 19) float32 0.164131\n",
      "y_train (100000,) int32 0.12489\n",
      "y_test (30000,) int32 0.102133333333\n"
     ]
    }
   ],
   "source": [
    "   \n",
    "# Print details to screen\n",
    "for key, val in data.iteritems():\n",
    "    print key, val.shape, val.dtype, val.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "if 'tX' in vars(): del tX\n",
    "if 'tY' in vars(): del tY\n",
    "if 'XY' in vars(): del XY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# splitting up to make the saving possible (https://github.com/numpy/numpy/issues/2396)\n",
    "data2 = {}\n",
    "data2['y_train'] = data['y_train']\n",
    "data2['y_test'] = data['y_test']\n",
    "data2['X_test'] = data['X_test']\n",
    "\n",
    "num_secs = np.ceil((data['X_train'].size * 4) / float((2**(32-1))))\n",
    "data2['X_train_split'] = np.array_split(data['X_train'], num_secs, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/michael/projects/engaged_hackathon_data/detection/train_test_patches/unbalanced_256.pkl\n"
     ]
    }
   ],
   "source": [
    "# save full dataset to disk\n",
    "\n",
    "print savedir + savename + '.pkl'\n",
    "\n",
    "# with open(savedir + savename + '.pkl', 'w') as f:\n",
    "#     pickle.dump(data, f, -1)\n",
    "scipy.io.savemat(savedir + savename, data2, do_compression=False, oned_as='column')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(30000, 256, 19)\n"
     ]
    }
   ],
   "source": [
    "print data['X_test'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print 378488*128*19*(32/8) / 1e9"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a subset of the train/test dataset and save to disk "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_test (2000, 256, 19)\n",
      "X_train (4000, 1, 256, 19)\n",
      "y_train (4000,)\n",
      "y_test (2000,)\n"
     ]
    }
   ],
   "source": [
    "# Take a subset of the full train/test split and save to disk\n",
    "maxi = {}\n",
    "maxi['train'] = 4000\n",
    "maxi['test'] = 2000\n",
    "\n",
    "small_data = {}\n",
    "\n",
    "for key in ['train', 'test']:\n",
    "    N = data['X_' + key].shape[0]\n",
    "    idxs = np.random.permutation(N)[:maxi[key]]\n",
    "    small_data['X_' + key] = data['X_' + key][idxs, :]\n",
    "    small_data['y_' + key] = data['y_' + key][idxs]\n",
    "\n",
    "#     data['X_' + key], data['y_' + key] = balance_classes(data['X_' + key], data['y_' + key])\n",
    "\n",
    "for key, val in small_data.iteritems():\n",
    "    print key, val.shape\n",
    "    \n",
    "scipy.io.savemat(savedir + savename + '_small.mat', small_data, do_compression=True, oned_as='column')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.5\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "print float(6) / float(4)\n",
    "print (6 + 4 // 2) // 4"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
