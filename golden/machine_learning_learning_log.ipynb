{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using gpu device 0: GeForce GTX 770 (CNMeM is disabled, cuDNN 5004)\n",
      "/home/michael/anaconda/lib/python2.7/site-packages/Theano-0.9.0.dev1-py2.7.egg/theano/tensor/signal/downsample.py:6: UserWarning: downsample module has been moved to the theano.tensor.signal.pool module.\n",
      "  \"downsample module has been moved to the theano.tensor.signal.pool module.\")\n"
     ]
    }
   ],
   "source": [
    "# creating spectrograms from all the files, and saving split labelled versions to disk ready for machine learning\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import cPickle as pickle\n",
    "import numpy as np\n",
    "import time\n",
    "import random \n",
    "import yaml\n",
    "\n",
    "from scipy.ndimage.interpolation import zoom\n",
    "\n",
    "#from data_helpers import load_annotations\n",
    "\n",
    "import nolearn\n",
    "import nolearn.lasagne\n",
    "import lasagne.layers\n",
    "\n",
    "from lasagne.layers import InputLayer, DimshuffleLayer\n",
    "from lasagne.layers import DenseLayer\n",
    "from lasagne.layers import NonlinearityLayer\n",
    "from lasagne.layers import DropoutLayer\n",
    "from lasagne.layers import Pool2DLayer as PoolLayer\n",
    "from lasagne.layers.dnn import Conv2DDNNLayer as ConvLayer\n",
    "from lasagne.nonlinearities import softmax, very_leaky_rectify as vlr\n",
    "import theano\n",
    "\n",
    "base = '/media/michael/Seagate/engage/alison_data/golden_set/'\n",
    "annotation_pkl_dir = base + 'extracted/annotations/'\n",
    "spec_pkl_dir = base + 'extracted/specs/'\n",
    "log_dir = base + 'ml_runs/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "HWW = 15\n",
    "SPEC_HEIGHT = 330\n",
    "LEARN_LOG = True\n",
    "DO_AUGMENTATION = True\n",
    "DO_BATCH_NORM = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14\n",
      "26\n"
     ]
    }
   ],
   "source": [
    "# loading splits\n",
    "splits = yaml.load(open(base + 'splits/folds.yaml'))\n",
    "train = splits[0] + splits[1]\n",
    "test = splits[2]\n",
    "                \n",
    "# load data and make list of specsamplers\n",
    "train_data = [[], []]\n",
    "test_data = [[], []]\n",
    "\n",
    "for fname in os.listdir(spec_pkl_dir):\n",
    "    \n",
    "    # load spectrogram and annotations\n",
    "    spec = pickle.load(open(spec_pkl_dir + fname))[:SPEC_HEIGHT, :]\n",
    "    annots, wav, sample_rate = pickle.load(open(annotation_pkl_dir + fname))\n",
    "        \n",
    "    # reshape annotations\n",
    "    for classname in annots:\n",
    "        factor = float(spec.shape[1]) / annots[classname].shape[0]\n",
    "        annots[classname] = zoom(annots[classname], factor)\n",
    "        \n",
    "    # create sampler\n",
    "    if not LEARN_LOG:\n",
    "        spec = np.log(0.001 + spec)\n",
    "        spec = spec - np.median(spec, axis=1, keepdims=True)\n",
    "\n",
    "    if fname in train:\n",
    "        train_data[0].append(spec)\n",
    "        train_data[1].append(annots['anthrop'])\n",
    "    elif fname in test:\n",
    "        test_data[0].append(spec)\n",
    "        test_data[1].append(annots['anthrop'])\n",
    "        \n",
    "print len(test_data[0])\n",
    "print len(train_data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class AllSpecs(object):\n",
    "    \n",
    "    def __init__(self, specs, labels, hww, do_aug, learn_log):\n",
    "        self.do_aug = do_aug\n",
    "        self.learn_log = learn_log\n",
    "        self.hww = hww\n",
    "        \n",
    "        blank_spec = np.zeros((specs[0].shape[0], hww))\n",
    "        self.specs = np.hstack([blank_spec] + specs + [blank_spec])[None, ...]\n",
    "        \n",
    "        blank_label = np.zeros(hww) - 1\n",
    "        self.labels = np.hstack([blank_label] + labels + [blank_label])\n",
    "        \n",
    "        which_spec = [ii * np.ones_like(lab) for ii, lab in enumerate(labels)]\n",
    "        self.which_spec = np.hstack([blank_label] + which_spec + [blank_label])\n",
    "        \n",
    "        if learn_log:\n",
    "            self.medians = np.zeros((len(specs), specs[0].shape[0], hww*2))\n",
    "            for idx, spec in enumerate(specs):\n",
    "                self.medians[idx] = np.median(spec, axis=1, keepdims=True)\n",
    "\n",
    "    def sample(self, num_per_class, seed=None):\n",
    "        \n",
    "        tic = time.time()\n",
    "        num_samples = num_per_class * 2\n",
    "        channels = self.specs.shape[0]\n",
    "        height = self.specs.shape[1]\n",
    "        \n",
    "        if seed is not None:\n",
    "            np.random.seed(seed)\n",
    "\n",
    "        X = np.zeros((num_samples, channels, height, self.hww*2), np.float32)\n",
    "        y = np.zeros(num_samples) * np.nan\n",
    "        if self.learn_log:\n",
    "            X_medians = np.zeros((num_samples, channels, height, self.hww*2), np.float32)\n",
    "        count = 0\n",
    "        \n",
    "        for cls in [0, 1]:\n",
    "            possible_locs = np.where(self.labels==cls)[0]\n",
    "\n",
    "            if len(possible_locs) >= num_per_class:\n",
    "                sampled_locs = np.random.choice(possible_locs, num_per_class, replace=False)\n",
    "\n",
    "                for loc in sampled_locs:\n",
    "                    X[count] = self.specs[:, :, (loc-self.hww):(loc+self.hww)]\n",
    "                    y[count] = cls\n",
    "                    if self.learn_log:\n",
    "                        which = self.which_spec[loc]\n",
    "                        X_medians[count] = self.medians[which]\n",
    "                    count += 1\n",
    "\n",
    "        # doing augmentation\n",
    "        if self.do_aug:\n",
    "            if self.learn_log:\n",
    "                X *= (1.0 + np.random.randn(num_samples, 1, 1, 1) * 0.1)\n",
    "            else:\n",
    "                X *= (1.0 + np.random.randn(num_samples, 1, 1, 1) * 0.1)\n",
    "                X += np.random.randn(num_samples, 1, 1, 1) * 0.05\n",
    "\n",
    "        if self.learn_log:\n",
    "            xb = {'input': X.astype(np.float32), 'input_med': X_medians.astype(np.float32)}\n",
    "            return xb, y.astype(np.int32)\n",
    "            \n",
    "        else:\n",
    "            # remove ones we couldn't get\n",
    "            return X.astype(np.float32), y.astype(np.int32)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_sampler = AllSpecs(train_data[0], train_data[1], HWW, DO_AUGMENTATION, LEARN_LOG)\n",
    "test_sampler = AllSpecs(test_data[0], test_data[1], HWW, False, LEARN_LOG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.04916e-05 (40, 1, 330, 30) 20\n",
      "1.23559e-05 (40, 1, 330, 30) 20\n",
      "3.78513e-05 (40, 1, 330, 30) 20\n",
      "1.0475e-05 (40, 1, 330, 30) 20\n",
      "1.96681e-05 (40, 1, 330, 30) 20\n",
      "2.07971e-05 (40, 1, 330, 30) 20\n",
      "1.38334e-06 (40, 1, 330, 30) 20\n",
      "3.23498e-05 (40, 1, 330, 30) 20\n",
      "5.51634e-05 (40, 1, 330, 30) 20\n",
      "4.58004e-05 (40, 1, 330, 30) 20\n",
      "2.28302e-05 (40, 1, 330, 30) 20\n",
      "1.79445e-05 (40, 1, 330, 30) 20\n",
      "1.13151e-05 (40, 1, 330, 30) 20\n",
      "1.40399e-05 (40, 1, 330, 30) 20\n",
      "3.97452e-05 (40, 1, 330, 30) 20\n",
      "2.63449e-05 (40, 1, 330, 30) 20\n",
      "1.72209e-05 (40, 1, 330, 30) 20\n",
      "3.13341e-05 (40, 1, 330, 30) 20\n",
      "3.36542e-05 (40, 1, 330, 30) 20\n",
      "2.20208e-05 (40, 1, 330, 30) 20\n",
      "2.21843e-05 (40, 1, 330, 30) 20\n",
      "9.74864e-06 (40, 1, 330, 30) 20\n",
      "3.2903e-05 (40, 1, 330, 30) 20\n",
      "3.68806e-05 (40, 1, 330, 30) 20\n",
      "1.97176e-05 (40, 1, 330, 30) 20\n",
      "2.8982e-05 (40, 1, 330, 30) 20\n",
      "8.49842e-06 (40, 1, 330, 30) 20\n",
      "1.74721e-05 (40, 1, 330, 30) 20\n",
      "2.50631e-05 (40, 1, 330, 30) 20\n",
      "5.63585e-05 (40, 1, 330, 30) 20\n",
      "1.23977e-05 (40, 1, 330, 30) 20\n",
      "6.20494e-06 (40, 1, 330, 30) 20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/michael/anaconda/lib/python2.7/site-packages/ipykernel/__main__.py:49: VisibleDeprecationWarning: using a non-integer number instead of an integer will result in an error in the future\n"
     ]
    }
   ],
   "source": [
    "class MyBatch(nolearn.lasagne.BatchIterator):\n",
    "    def __iter__(self):\n",
    "        bs = self.batch_size\n",
    "        for _ in range(32):\n",
    "            yield self.X.sample(bs)\n",
    "\n",
    "            \n",
    "class MyBatchTest(nolearn.lasagne.BatchIterator):\n",
    "    def __iter__(self):\n",
    "        bs = self.batch_size\n",
    "        for idx in range(128):\n",
    "            yield self.X.sample(bs, seed=idx)\n",
    "                \n",
    "\n",
    "mb = MyBatch(20)\n",
    "mb.X = train_sampler\n",
    "\n",
    "for count, (xx, yy) in enumerate(mb):\n",
    "    if LEARN_LOG:\n",
    "        print xx['input'].min(), xx['input_med'].shape, yy.sum()\n",
    "    else:\n",
    "        print xx.shape, yy.shape, yy.sum(), xx.sum()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class MyTrainSplit(nolearn.lasagne.TrainSplit):\n",
    "    # custom data split\n",
    "    def __call__(self, data, Yb, net):\n",
    "        return train_sampler, test_sampler, None, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# from lasagne.nonlinearities import elu as vlr\n",
    "from lasagne.nonlinearities import softmax, elu as vlr\n",
    "from lasagne.layers import batch_norm, ElemwiseSumLayer, ExpressionLayer, DimshuffleLayer\n",
    "from helpers import Log1Plus, ForgetSizeLayer\n",
    "import theano.tensor as T\n",
    "\n",
    "NUM_FILTERS = 32\n",
    "\n",
    "if not DO_BATCH_NORM:\n",
    "    batch_norm = lambda x: x\n",
    "net = {}\n",
    "\n",
    "# main input layer, then logged\n",
    "net['input'] = InputLayer((None, 1, SPEC_HEIGHT, HWW*2), name='input')\n",
    "\n",
    "if LEARN_LOG:\n",
    "    off = lasagne.init.Constant(0.5)\n",
    "    mult = lasagne.init.Constant(1.0)\n",
    "\n",
    "    net['input_logged'] = Log1Plus(net['input'], off, mult)\n",
    "\n",
    "    # logging the median and multiplying by -1\n",
    "    net['input_med'] = InputLayer((None, 1, SPEC_HEIGHT, HWW*2), name='input_med')\n",
    "    net['med_logged'] = Log1Plus(net['input_med'], off=net['input_logged'].off, mult=net['input_logged'].mult)\n",
    "    net['med_logged'] = ExpressionLayer(net['med_logged'], lambda X: -X)\n",
    "\n",
    "    # summing the logged input with the negative logged median\n",
    "    net['input'] = ElemwiseSumLayer((net['input_logged'], net['med_logged']))\n",
    "\n",
    "net['conv1_1'] = batch_norm(\n",
    "    ConvLayer(net['input'], NUM_FILTERS, (spec.shape[0] - 1, 6), nonlinearity=vlr))\n",
    "net['pool1'] = PoolLayer(net['conv1_1'], pool_size=(2, 2), stride=(2, 2), mode='max')\n",
    "net['pool1'] = DropoutLayer(net['pool1'], p=0.5)\n",
    "net['conv1_2'] = batch_norm(ConvLayer(net['pool1'], NUM_FILTERS, (1, 3), nonlinearity=vlr))\n",
    "# net['pool2'] = PoolLayer(net['conv1_2'], pool_size=(1, 2), stride=(1, 1))\n",
    "net['pool2'] = DropoutLayer(net['conv1_2'], p=0.5)\n",
    "\n",
    "net['fc6'] = batch_norm(DenseLayer(net['pool2'], num_units=64, nonlinearity=vlr))\n",
    "net['fc6'] = DropoutLayer(net['fc6'], p=0.5)\n",
    "net['fc7'] = batch_norm(DenseLayer(net['fc6'], num_units=64, nonlinearity=vlr))\n",
    "net['fc7'] = DropoutLayer(net['fc7'], p=0.5)\n",
    "net['fc8'] = DenseLayer(net['fc7'], num_units=2, nonlinearity=None)\n",
    "net['prob'] = NonlinearityLayer(net['fc8'], softmax)\n",
    "\n",
    "net = nolearn.lasagne.NeuralNet(\n",
    "    layers=net['prob'],\n",
    "    max_epochs=500,\n",
    "    update=lasagne.updates.adam,\n",
    "    update_learning_rate=0.0001,\n",
    "#     update_momentum=0.975,\n",
    "    verbose=1,\n",
    "    batch_iterator_train=MyBatch(128),\n",
    "    batch_iterator_test=MyBatchTest(128),\n",
    "    train_split=MyTrainSplit(None),\n",
    "    check_input=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Neural Network with 91332 learnable parameters\n",
      "\n",
      "## Layer information\n",
      "\n",
      "  #  name       size\n",
      "---  ---------  --------\n",
      "  0  input      1x330x30\n",
      "  1             1x330x30\n",
      "  2  input_med  1x330x30\n",
      "  3             1x330x30\n",
      "  4             1x330x30\n",
      "  5             1x330x30\n",
      "  6             32x2x25\n",
      "  7             32x2x25\n",
      "  8             32x2x25\n",
      "  9             32x1x12\n",
      " 10             32x1x12\n",
      " 11             32x1x10\n",
      " 12             32x1x10\n",
      " 13             32x1x10\n",
      " 14             32x1x10\n",
      " 15             64\n",
      " 16             64\n",
      " 17             64\n",
      " 18             64\n",
      " 19             64\n",
      " 20             64\n",
      " 21             64\n",
      " 22             64\n",
      " 23             2\n",
      " 24             2\n",
      "\n",
      "  epoch    train loss    valid loss    train/val    valid acc  dur\n",
      "-------  ------------  ------------  -----------  -----------  -----\n",
      "      1       \u001b[36m1.05796\u001b[0m       \u001b[32m0.53622\u001b[0m      1.97299      0.71683  6.03s\n",
      "      2       \u001b[36m0.86872\u001b[0m       \u001b[32m0.47349\u001b[0m      1.83473      0.78467  6.05s\n",
      "      3       \u001b[36m0.74832\u001b[0m       \u001b[32m0.43822\u001b[0m      1.70763      0.81793  6.04s\n",
      "      4       \u001b[36m0.67319\u001b[0m       \u001b[32m0.42624\u001b[0m      1.57936      0.81943  6.05s\n",
      "      5       \u001b[36m0.59284\u001b[0m       0.43549      1.36132      0.80762  6.04s\n",
      "      6       \u001b[36m0.54748\u001b[0m       \u001b[32m0.42350\u001b[0m      1.29274      0.81689  6.04s\n",
      "      7       \u001b[36m0.49956\u001b[0m       0.47348      1.05508      0.79581  6.03s\n",
      "      8       \u001b[36m0.46923\u001b[0m       \u001b[32m0.41777\u001b[0m      1.12317      0.81976  6.08s\n",
      "      9       \u001b[36m0.45361\u001b[0m       0.46751      0.97026      0.80692  6.03s\n",
      "     10       \u001b[36m0.40385\u001b[0m       0.46210      0.87394      0.81271  6.03s\n",
      "     11       0.41224       0.44678      0.92270      0.81461  6.06s\n",
      "     12       \u001b[36m0.39906\u001b[0m       0.45244      0.88200      0.81937  6.03s\n",
      "     13       \u001b[36m0.37246\u001b[0m       0.48570      0.76684      0.80923  6.03s\n",
      "     14       \u001b[36m0.35750\u001b[0m       0.51658      0.69204      0.80270  6.05s\n",
      "     15       \u001b[36m0.34721\u001b[0m       0.46137      0.75257      0.81815  6.03s\n",
      "     16       \u001b[36m0.33674\u001b[0m       0.48630      0.69244      0.80972  6.10s\n",
      "     17       \u001b[36m0.33429\u001b[0m       0.51524      0.64880      0.80643  6.25s\n",
      "     18       \u001b[36m0.33328\u001b[0m       0.45039      0.73997      0.82193  6.15s\n",
      "     19       \u001b[36m0.32527\u001b[0m       0.46783      0.69526      0.82526  6.16s\n",
      "     20       \u001b[36m0.30456\u001b[0m       0.44629      0.68242      0.81827  6.24s\n"
     ]
    }
   ],
   "source": [
    "net.fit(None, None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.46714729]\n",
      "[ 0.46714729]\n",
      "[ 1.03194082]\n",
      "[ 1.03194082]\n"
     ]
    }
   ],
   "source": [
    "print net.layers_['log1plus1'].off.get_value()\n",
    "print net.layers_['log1plus3'].off.get_value()\n",
    "print net.layers_['log1plus1'].mult.get_value()\n",
    "print net.layers_['log1plus3'].mult.get_value()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def plot_loss(net):\n",
    "    train_loss = [row['train_loss'] for row in net.train_history_]\n",
    "    valid_loss = [row['valid_loss'] for row in net.train_history_]\n",
    "    plt.plot(train_loss, label='train loss')\n",
    "    plt.plot(valid_loss, label='valid loss')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.ylabel('loss')\n",
    "    plt.legend(loc='best')\n",
    "    return plt\n",
    "plot_loss(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y = [0.197185359589, 0.518698018591, 0.581381482387, 0.664551736791, 0.699983182485]\n",
    "x = [100, 500, 1000, 2500, 5000]\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
