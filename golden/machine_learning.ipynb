{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using gpu device 0: GeForce GTX 770 (CNMeM is disabled, cuDNN 5004)\n",
      "/home/michael/anaconda/lib/python2.7/site-packages/Theano-0.9.0.dev1-py2.7.egg/theano/tensor/signal/downsample.py:6: UserWarning: downsample module has been moved to the theano.tensor.signal.pool module.\n",
      "  \"downsample module has been moved to the theano.tensor.signal.pool module.\")\n"
     ]
    }
   ],
   "source": [
    "# creating spectrograms from all the files, and saving split labelled versions to disk ready for machine learning\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import cPickle as pickle\n",
    "import numpy as np\n",
    "import time\n",
    "import random \n",
    "\n",
    "from scipy.ndimage.interpolation import zoom\n",
    "\n",
    "#from data_helpers import load_annotations\n",
    "\n",
    "import nolearn\n",
    "import nolearn.lasagne\n",
    "import lasagne.layers\n",
    "\n",
    "from lasagne.layers import InputLayer, DimshuffleLayer\n",
    "from lasagne.layers import DenseLayer\n",
    "from lasagne.layers import NonlinearityLayer\n",
    "from lasagne.layers import DropoutLayer\n",
    "from lasagne.layers import Pool2DLayer as PoolLayer\n",
    "from lasagne.layers.dnn import Conv2DDNNLayer as ConvLayer\n",
    "from lasagne.nonlinearities import softmax, very_leaky_rectify as vlr\n",
    "import theano\n",
    "\n",
    "base = '/media/michael/Seagate/engage/alison_data/golden_set/'\n",
    "annotation_pkl_dir = base + 'extracted/annotations/'\n",
    "spec_pkl_dir = base + 'extracted/specs/'\n",
    "log_dir = base + 'ml_runs/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "HWW = 15\n",
    "SPEC_HEIGHT = 330"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create a class for the spectrograms, from which we can get minbatches of data\n",
    "class SpecSampler(object):\n",
    "    \n",
    "    def _ensure_correct_shape_spec(self, spec):\n",
    "        if spec.ndim == 2:\n",
    "            return spec[None, ...]\n",
    "        else:\n",
    "            return spec\n",
    "    \n",
    "    def __init__(self, specs, labels, hww):       \n",
    "        self.specs = np.vstack(self._ensure_correct_shape_spec(spec) for spec in specs)\n",
    "        self.labels = labels\n",
    "        self.hww = hww\n",
    "    \n",
    "    def _ensure_shape(self, X):\n",
    "        if X.shape[2] < self.hww * 2:\n",
    "            return np.pad(X, ((0, 0), (0, 0), (0, self.hww * 2 - X.shape[2])), 'constant')\n",
    "        else:\n",
    "            return X\n",
    "    \n",
    "    def sample(self, num_per_class, seed=None):\n",
    "        \n",
    "        tic = time.time()\n",
    "        num_samples = num_per_class * 2\n",
    "        channels = self.specs.shape[0]\n",
    "        height = self.specs.shape[1]\n",
    "        \n",
    "        if seed is not None:\n",
    "            np.random.seed(seed)\n",
    "\n",
    "        X = np.zeros((num_samples, channels, height, self.hww*2), np.float32)\n",
    "        y = np.zeros(num_samples) * np.nan\n",
    "        count = 0\n",
    "        tic = time.time()\n",
    "        \n",
    "        for cls in [0, 1]:\n",
    "            possible_locs = np.where(self.labels==cls)[0]\n",
    "\n",
    "            if len(possible_locs) >= num_per_class:\n",
    "                sampled_locs = np.random.choice(possible_locs, num_per_class, replace=False)\n",
    "\n",
    "                for loc in sampled_locs:\n",
    "                    X[count] = self._ensure_shape(\n",
    "                        self.specs[:, :, (loc-self.hww):(loc+self.hww)])\n",
    "                    y[count] = cls\n",
    "                    count += 1\n",
    "\n",
    "        # remove ones we couldn't get\n",
    "        to_remove = np.isnan(y)\n",
    "        return X[~to_remove], y[~to_remove]\n",
    "        \n",
    "        \n",
    "#ss = SpecSampler([np.log((0.001+ melspec))[None, :100, :]], {0: bio_zoomed}, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# load data and make list of specsamplers\n",
    "samplers = []\n",
    "\n",
    "for fname in os.listdir(spec_pkl_dir):\n",
    "    \n",
    "    # load spectrogram and annotations\n",
    "    spec = pickle.load(open(spec_pkl_dir + fname))[:SPEC_HEIGHT, :]\n",
    "    annots, wav, sample_rate = pickle.load(\n",
    "        open(annotation_pkl_dir + fname))\n",
    "        \n",
    "    # reshape annotations\n",
    "    for classname in annots:\n",
    "        factor = float(spec.shape[1]) / annots[classname].shape[0]\n",
    "        annots[classname] = zoom(annots[classname], factor)\n",
    "        \n",
    "    # create sampler\n",
    "    logspec = np.log(0.001 + spec)\n",
    "    logspec_med = logspec - np.median(logspec, axis=1, keepdims=True)\n",
    "    spec_stack = [logspec_med]#, np.random.rand(*logspec.shape)]\n",
    "    ss = SpecSampler(spec_stack, annots['biotic'], HWW)\n",
    "    samplers.append(ss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class MyBatch(nolearn.lasagne.BatchIterator):\n",
    "    def __iter__(self):\n",
    "        bs = self.batch_size\n",
    "        for sampler in self.X:\n",
    "            xb, yb = sampler.sample(bs)\n",
    "            num = xb.shape[0]\n",
    "            xb *= (1.0 + np.random.randn(num, 1, 1, 1) * 0.1)\n",
    "            xb += np.random.randn(num, 1, 1, 1) * 0.05\n",
    "            yield xb.astype(np.float32), yb.astype(np.int32)\n",
    "\n",
    "            \n",
    "class MyBatchTest(nolearn.lasagne.BatchIterator):\n",
    "    def __iter__(self):\n",
    "        bs = self.batch_size\n",
    "        for sampler in self.X:\n",
    "            xb, yb = sampler.sample(bs, seed=10)\n",
    "            yield xb.astype(np.float32), yb.astype(np.int32)\n",
    "\n",
    "            \n",
    "class MyTrainSplit(nolearn.lasagne.TrainSplit):\n",
    "    # custom data split\n",
    "    def __call__(self, data, Yb, net):\n",
    "        return samplers[:50], samplers[50:], None, None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# from lasagne.nonlinearities import elu as vlr\n",
    "from lasagne.nonlinearities import softmax, very_leaky_rectify as vlr\n",
    "\n",
    "net = {}\n",
    "net['input'] = InputLayer((None, len(spec_stack), SPEC_HEIGHT, HWW*2))\n",
    "\n",
    "net['conv1_1'] = ConvLayer(net['input'], 80, (spec.shape[0] - 5, 6), nonlinearity=vlr)\n",
    "net['pool1'] = PoolLayer(net['conv1_1'], pool_size=(4, 3), stride=(1, 3))\n",
    "net['pool1'] = DropoutLayer(net['pool1'], p=0.5)\n",
    "net['conv1_2'] = ConvLayer(net['pool1'], 80, (1, 3), nonlinearity=vlr)\n",
    "net['pool2'] = PoolLayer(net['conv1_2'], pool_size=(1, 2), stride=(1, 1))\n",
    "\n",
    "net['fc6'] = DenseLayer(net['pool2'], num_units=256, nonlinearity=vlr)\n",
    "net['fc6'] = DropoutLayer(net['fc6'], p=0.5)\n",
    "net['fc7'] = DenseLayer(net['fc6'], num_units=256, nonlinearity=vlr)\n",
    "net['fc7'] = DropoutLayer(net['fc7'], p=0.5)\n",
    "net['fc8'] = DenseLayer(net['fc7'], num_units=2, nonlinearity=None)\n",
    "net['prob'] = NonlinearityLayer(net['fc8'], softmax)\n",
    "\n",
    "net = nolearn.lasagne.NeuralNet(\n",
    "    layers=net['prob'],\n",
    "    max_epochs=500,\n",
    "    update=lasagne.updates.nesterov_momentum,\n",
    "    update_learning_rate=0.001,\n",
    "    update_momentum=0.975,\n",
    "    verbose=1,\n",
    "    batch_iterator_train=MyBatch(128),\n",
    "    batch_iterator_test=MyBatchTest(128),\n",
    "    train_split=MyTrainSplit(None),\n",
    "    check_input=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Neural Network with 549122 learnable parameters\n",
      "\n",
      "## Layer information\n",
      "\n",
      "  #    name  size\n",
      "---  ------  --------\n",
      "  0          1x330x30\n",
      "  1          80x6x25\n",
      "  2          80x3x8\n",
      "  3          80x3x8\n",
      "  4          80x3x6\n",
      "  5          80x3x5\n",
      "  6          256\n",
      "  7          256\n",
      "  8          256\n",
      "  9          256\n",
      " 10          2\n",
      " 11          2\n",
      "\n",
      "  epoch    train loss    valid loss    train/val    valid acc  dur\n",
      "-------  ------------  ------------  -----------  -----------  -----\n",
      "      1       \u001b[36m0.74585\u001b[0m       \u001b[32m0.69535\u001b[0m      1.07263      0.54308  8.66s\n",
      "      2       \u001b[36m0.71622\u001b[0m       \u001b[32m0.65600\u001b[0m      1.09180      0.72691  8.69s\n",
      "      3       \u001b[36m0.64407\u001b[0m       \u001b[32m0.65040\u001b[0m      0.99027      0.72920  8.87s\n",
      "      4       \u001b[36m0.64079\u001b[0m       0.65127      0.98390      0.72358  8.38s\n",
      "      5       \u001b[36m0.62438\u001b[0m       0.65671      0.95077      0.71978  8.71s\n"
     ]
    }
   ],
   "source": [
    "net.fit(samplers, None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def plot_loss(net):\n",
    "    train_loss = [row['train_loss'] for row in net.train_history_]\n",
    "    valid_loss = [row['valid_loss'] for row in net.train_history_]\n",
    "    plt.plot(train_loss, label='train loss')\n",
    "    plt.plot(valid_loss, label='valid loss')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.ylabel('loss')\n",
    "    plt.legend(loc='best')\n",
    "    return plt\n",
    "plot_loss(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
